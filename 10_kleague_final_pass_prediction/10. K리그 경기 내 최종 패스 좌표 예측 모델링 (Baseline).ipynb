{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"gpuType":"V5E1","mount_file_id":"18RrnWl2xRc2c3cbBZvAFl85MlECH-Ha6","authorship_tag":"ABX9TyNuLU3xNocJZ1xuWkdaX8eJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# 1. Import"],"metadata":{"id":"Md7JpNQpLNhX"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","%cd \"/content/drive/MyDrive/데이터 분석/projects/ML_portfolio/10_kleague_final_pass_prediction\""],"metadata":{"id":"VV6_s3glLjVk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghfBfk0p_TEn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","import torch\n","from torch import nn\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","source":["# 2. Hyperparameter Setting"],"metadata":{"id":"kNa67kkMLRG0"}},{"cell_type":"code","source":["TRAIN_PATH = \"Data/train.csv\"\n","BATCH_SIZE = 64\n","EPOCHS = 30\n","LR = 1e-3\n","HIDDEN_DIM = 96\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", DEVICE)"],"metadata":{"id":"tMKajnMPLPJN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Data Load & Preprocessing"],"metadata":{"id":"AUBTtyfwLZ4z"}},{"cell_type":"code","source":["# event 단순화 매핑 함수\n","def simplify_event(t: str) -> str:\n","    if t in [\"Pass\", \"Pass_Freekick\", \"Pass_Corner\"]:\n","        return \"Pass\"\n","    if t == \"Carry\":\n","        return \"Carry\"\n","    if t in [\"Duel\", \"Tackle\", \"Interception\", \"Recovery\"]:\n","        return \"Duel_Turnover\"\n","    if t == \"Cross\":\n","        return \"Cross\"\n","    if t.startswith(\"Shot\"):\n","        return \"Shot\"\n","    if t == \"Penalty Kick\":\n","        return \"Shot\"\n","    if t in [\"Clearance\", \"Aerial Clearance\"]:\n","        return \"Clearance\"\n","    if t in [\"Catch\", \"Parry\", \"Goal Kick\", \"Keeper Rush-Out\"]:\n","        return \"GK_Action\"\n","    if t in [\"Block\", \"Deflection\", \"Intervention\", \"Hit\"]:\n","        return \"Deflect_Block\"\n","    if t == \"Throw-In\":\n","        return \"SetPiece\"\n","    if t in [\"Goal\", \"Own Goal\"]:\n","        return \"Goal_Event\"\n","    if t in [\"Error\", \"Out\", \"Foul\", \"Foul_Throw\", \"Handball_Foul\", \"Offside\"]:\n","        return \"Error_Out\"\n","    return \"Misc\"\n","\n","def build_episode_sequence(g: pd.DataFrame, for_train: bool = True):\n","    \"\"\"\n","    한 game_episode의 데이터프레임 g를 받아서\n","    - seq: [T, F] numeric feature 시퀀스\n","    - target: [2] (정규화된 end_x, end_y)\n","    을 반환하는 함수.\n","    \"\"\"\n","    g = g.reset_index(drop=True).copy()\n","    if len(g) < 2:\n","        return None, None, None\n","\n","    # 기본 좌표 & 시간\n","    sx = g[\"start_x\"].values\n","    sy = g[\"start_y\"].values\n","    t  = g[\"time_seconds\"].values\n","\n","    # 이동량\n","    dx = np.diff(sx, prepend=sx[0])\n","    dy = np.diff(sy, prepend=sy[0])\n","    dist = np.sqrt(dx**2 + dy**2)\n","    angle = np.arctan2(dy, dx)  # -pi ~ pi\n","\n","    # 시간차\n","    dt = np.diff(t, prepend=t[0])\n","    dt[dt < 0] = 0  # 역전 방지\n","\n","    # 누적 이동량\n","    cum_dx = np.cumsum(dx)\n","    cum_dy = np.cumsum(dy)\n","    move_norm = np.sqrt(cum_dx**2 + cum_dy**2)\n","\n","    # episode 내부 상대 위치\n","    T = len(g)\n","    step_idx = np.arange(T)\n","    step_idx_norm = step_idx / (T - 1) if T > 1 else np.zeros(T)\n","\n","    # 상대 시간 (0~1)\n","    t_min, t_max = t.min(), t.max()\n","    time_rel = (t - t_min) / (t_max - t_min) if t_max > t_min else np.zeros(T)\n","\n","    # -------------------------\n","    # 정규화 (EDA 기반 스케일링)\n","    # -------------------------\n","    sx_norm = sx / 105.0\n","    sy_norm = sy / 68.0\n","\n","    dx_norm = dx / 40.0\n","    dy_norm = dy / 40.0\n","    dist_norm = dist / 40.0\n","    angle_norm = angle / np.pi\n","\n","    # dt: 3초를 넘는 경우는 1로 클립\n","    dt_norm = np.clip(dt / 3.0, 0, 1)\n","\n","    cum_dx_norm = cum_dx / 60.0\n","    cum_dy_norm = cum_dy / 60.0\n","    move_norm_norm = move_norm / 60.0\n","\n","    # feature matrix 구성: [T, F]\n","    feats = np.stack([\n","        sx_norm, sy_norm,         # 2\n","        dx_norm, dy_norm,         # 2\n","        dist_norm,                # 1\n","        angle_norm,               # 1\n","        dt_norm,                  # 1\n","        cum_dx_norm, cum_dy_norm, # 2\n","        move_norm_norm,           # 1\n","        step_idx_norm,            # 1\n","        time_rel                  # 1\n","    ], axis=1).astype(\"float32\")  # 총 12차원\n","\n","    event_idx = g[\"event_s\"].apply(lambda x: event2idx[x]).values.astype(\"int64\")\n","\n","    # target: 마지막 end_x, end_y\n","    target = None\n","    if for_train:\n","        ex = g[\"end_x\"].values[-1] / 105.0\n","        ey = g[\"end_y\"].values[-1] / 68.0\n","        target = np.array([ex, ey], dtype=\"float32\")\n","\n","    return feats, event_idx, target"],"metadata":{"id":"nqtPDf7YLU3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(TRAIN_PATH)\n","df = df.sort_values([\"game_episode\", \"time_seconds\"]).reset_index(drop=True)\n","\n","df[\"event_s\"] = df[\"type_name\"].astype(str).apply(simplify_event)\n","\n","event_vocab = sorted(df[\"event_s\"].unique())\n","event2idx = {ev: i for i, ev in enumerate(event_vocab)}\n","\n","print(\"Event vocabulary size:\", len(event2idx))\n","print(event2idx)"],"metadata":{"id":"LV90W8Sf6kua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["episodes = []\n","events = []\n","targets = []\n","\n","for _, g in tqdm(df.groupby(\"game_episode\")):\n","    seq, ev, tgt = build_episode_sequence(g, for_train=True)\n","    if seq is None:\n","        continue\n","    episodes.append(seq)\n","    events.append(ev)\n","    targets.append(tgt)\n","\n","print(\"Episode count:\", len(episodes))\n","print(\"Example shape:\", episodes[0].shape)"],"metadata":{"id":"3kJjn9iK6PFi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","100%|██████████| 15435/15435 [00:09<00:00, 1553.36it/s]\n","\n","Episode count: 15428\n","Example shape: (49, 12)\n","```\n","\n"],"metadata":{"id":"Dr3m0dj9cm3M"}},{"cell_type":"markdown","source":["# 4. Custom Dataset / DataLoader 정의 및 Validation 분할"],"metadata":{"id":"RhLSjdqvL0tb"}},{"cell_type":"code","source":["class EpisodeDataset(Dataset):\n","    def __init__(self, episodes, events, targets):\n","        self.episodes = episodes\n","        self.events = events\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.episodes)\n","\n","    def __getitem__(self, idx):\n","        seq = torch.tensor(self.episodes[idx])     # [T, 12]\n","        ev  = torch.tensor(self.events[idx])       # [T]\n","        tgt = torch.tensor(self.targets[idx])      # [2]\n","        length = seq.size(0)\n","        return seq, ev, length, tgt\n","\n","def collate_fn(batch):\n","    seqs, evs, lengths, tgts = zip(*batch)\n","\n","    lengths = torch.tensor(lengths, dtype=torch.long)\n","    tgts = torch.stack(tgts, dim=0)\n","\n","    padded_seq = pad_sequence(seqs, batch_first=True)             # [B, T, 12]\n","    padded_ev  = pad_sequence(evs, batch_first=True, padding_value=0)  # [B, T]\n","\n","    return padded_seq, padded_ev, lengths, tgts\n","\n","# 에피소드 단위 train / valid split\n","idx_train, idx_valid = train_test_split(\n","    np.arange(len(episodes)), test_size=0.2, random_state=42\n",")\n","\n","episodes_train = [episodes[i] for i in idx_train]\n","events_train   = [events[i]   for i in idx_train]\n","targets_train  = [targets[i]  for i in idx_train]\n","\n","episodes_valid = [episodes[i] for i in idx_valid]\n","events_valid   = [events[i]   for i in idx_valid]\n","targets_valid  = [targets[i]  for i in idx_valid]\n","\n","train_loader = DataLoader(\n","    EpisodeDataset(episodes_train, events_train, targets_train),\n","    batch_size=64, shuffle=True, collate_fn=collate_fn\n",")\n","\n","valid_loader = DataLoader(\n","    EpisodeDataset(episodes_valid, events_valid, targets_valid),\n","    batch_size=64, shuffle=False, collate_fn=collate_fn\n",")\n","\n","print(\"train episodes:\", len(episodes_train), \"valid episodes:\", len(episodes_valid))"],"metadata":{"id":"BJeZH7DALgcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","train episodes: 12342 valid episodes: 3086\n","```\n","\n"],"metadata":{"id":"rNDeZMGBckaW"}},{"cell_type":"markdown","source":["# 5. LSTM 베이스라인 + 이동량 모델 정의"],"metadata":{"id":"p2z7Qk4UL4or"}},{"cell_type":"code","source":["class LSTMWithEventEmbedding(nn.Module):\n","    def __init__(self, num_feats=12, hidden_dim=96, emb_dim=6):\n","        super().__init__()\n","\n","        self.event_emb = nn.Embedding(len(event2idx), emb_dim)\n","\n","        input_dim = num_feats + emb_dim   # 12 + 6 = 18\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=1,\n","            batch_first=True\n","        )\n","\n","        self.fc = nn.Linear(hidden_dim, 2)\n","\n","    def forward(self, seq, ev, lengths):\n","        ev_e = self.event_emb(ev)               # [B, T, 6]\n","        x = torch.cat([seq, ev_e], dim=2)       # [B, T, 18]\n","\n","        packed = pack_padded_sequence(\n","            x, lengths.cpu(),\n","            batch_first=True,\n","            enforce_sorted=False\n","        )\n","        _, (h, _) = self.lstm(packed)\n","        h = h[-1]                               # [B, hidden_dim]\n","\n","        return self.fc(h)"],"metadata":{"id":"nookOJsHL3LW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. 모델 학습 및 검증"],"metadata":{"id":"P4f3f-4rL9Lx"}},{"cell_type":"code","source":["model = LSTMWithEventEmbedding().to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()\n","\n","best_dist = float(\"inf\")\n","best_state = None\n","\n","for epoch in range(1, EPOCHS + 1):\n","    # --- Train ---\n","    model.train()\n","    total_loss = 0.0\n","\n","    for X, EV, L, y in tqdm(train_loader):\n","        X, EV, L, y = X.to(DEVICE), EV.to(DEVICE), L.to(DEVICE), y.to(DEVICE)\n","\n","        optimizer.zero_grad()\n","        pred = model(X, EV, L)\n","        loss = criterion(pred, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * X.size(0)\n","\n","    train_loss = total_loss / len(train_loader.dataset)\n","\n","    # --- Valid: 평균 유클리드 거리 ---\n","    model.eval()\n","    dists = []\n","\n","    with torch.no_grad():\n","        for X, EV, L, y in tqdm(valid_loader):\n","            X, EV, L, y = X.to(DEVICE), EV.to(DEVICE), L.to(DEVICE), y.to(DEVICE)\n","\n","            pred = model(X, EV, L)\n","\n","            pred_np = pred.cpu().numpy()\n","            true_np = y.cpu().numpy()\n","\n","            px = pred_np[:,0] * 105\n","            py = pred_np[:,1] * 68\n","            tx = true_np[:,0] * 105\n","            ty = true_np[:,1] * 68\n","\n","            dist = np.sqrt((px - tx)**2 + (py - ty)**2)\n","            dists.append(dist)\n","\n","    mean_dist = np.concatenate(dists).mean()\n","\n","    print(f\"[Epoch {epoch}] train={train_loss:.4f} | valid_dist={mean_dist:.4f}\")\n","\n","    if mean_dist < best_dist:\n","        best_dist = mean_dist\n","        best_state = model.state_dict()\n","        print(f\" → Best updated: {best_dist:.4f}\")"],"metadata":{"id":"DFVil3J5L6vC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# baseline\n","[Epoch 18] train_loss=0.0304 | valid_mean_dist=16.6270\n"," --> Best model updated! (dist=16.6270)\n","\n","# hidden_dim=96으로 수정 후\n","[Epoch 27] train_loss=0.0282 | valid_mean_dist=16.3134\n"," --> Best model updated! (dist=16.3134)\n","\n","# hidden_dim=96으로 수정 + 컬럼 스케일링 범위 조정\n","[Epoch 28] train_loss=0.0287 | valid_mean_dist=16.2878\n"," --> Best model updated! (dist=16.2878)\n","\n","# Event Embedding 추가한 후\n","[Epoch 30] train_loss=0.0267 | valid_mean_dist=15.8609\n"," --> Best model updated! (dist=15.8609)\n","```\n","\n"],"metadata":{"id":"_3Z2XL0_de-y"}},{"cell_type":"markdown","source":["# 7. 평가 데이터셋 추론"],"metadata":{"id":"lsUazbcbMZ_a"}},{"cell_type":"code","source":["# ---------------------------------------------------------\n","# 0. 준비: 모델 로드\n","# ---------------------------------------------------------\n","model.load_state_dict(best_state)\n","model.eval()\n","\n","# ---------------------------------------------------------\n","# 1. Test 메타 로드\n","# ---------------------------------------------------------\n","test_meta = pd.read_csv(\"Data/test.csv\")\n","submission = pd.read_csv(\"Data/sample_submission.csv\")\n","submission = submission.merge(test_meta, on=\"game_episode\", how=\"left\")\n","\n","# ---------------------------------------------------------\n","# 2. test 파일 전체 캐싱 — 속도 10~20배 빨라짐\n","# ---------------------------------------------------------\n","def load_all_test_files(test_meta, base_dir=\"Data\"):\n","    cache = {}\n","    for path in tqdm(test_meta[\"path\"].unique(), desc=\"Loading test files\"):\n","        clean = path[1:]              # \"/XXX.csv\" → \"XXX.csv\"\n","        full_path = base_dir + clean\n","        df = pd.read_csv(full_path)\n","        cache[path] = df\n","    return cache\n","\n","file_cache = load_all_test_files(test_meta)\n","\n","# ---------------------------------------------------------\n","# 3. Inference\n","# ---------------------------------------------------------\n","preds_x, preds_y = [], []\n","\n","with torch.no_grad():\n","    for _, row in tqdm(submission.iterrows(), total=len(submission), desc=\"Inference\"):\n","\n","        # 캐시에서 해당 episode 로드 (매우 빠름)\n","        g = file_cache[row[\"path\"]].copy()\n","\n","        # event_s와 그룹 매핑 적용\n","        g[\"event_s\"] = g[\"type_name\"].astype(str).apply(simplify_event)\n","\n","        # Step 2 build_episode_sequence 호출\n","        seq, ev, _ = build_episode_sequence(g, for_train=False)\n","\n","        # ---- 방어: seq가 None이면 fallback 좌표 사용 ----\n","        if seq is None:\n","            sx = g[\"start_x\"].values[-1] / 105.0\n","            sy = g[\"start_y\"].values[-1] / 68.0\n","            pred_norm = np.array([sx, sy], dtype=\"float32\")\n","\n","        else:\n","            # 텐서화\n","            X = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(DEVICE)   # [1, T, 12]\n","            EV = torch.tensor(ev, dtype=torch.long).unsqueeze(0).to(DEVICE)      # [1, T]\n","            L  = torch.tensor([seq.shape[0]], dtype=torch.long).to(DEVICE)       # [1]\n","\n","            # 모델 추론\n","            pred = model(X, EV, L)[0].cpu().numpy()\n","            pred_norm = pred\n","\n","        # 정규화 좌표 → 실제 좌표\n","        preds_x.append(pred_norm[0] * 105.0)\n","        preds_y.append(pred_norm[1] * 68.0)"],"metadata":{"id":"c7Kr_PH6MAGc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","Loading test files: 100%|██████████| 2414/2414 [08:54<00:00,  4.51it/s]\n","Inference: 100%|██████████| 2414/2414 [00:05<00:00, 470.82it/s]\n","```\n","\n"],"metadata":{"id":"4lyHowTYBK0B"}},{"cell_type":"markdown","source":["# 8. 제출 Submission 생성"],"metadata":{"id":"MIMzakLGMbf7"}},{"cell_type":"code","source":["submission[\"end_x\"] = preds_x\n","submission[\"end_y\"] = preds_y\n","\n","save_path = \"Data/step2_submit.csv\"\n","submission[[\"game_episode\", \"end_x\", \"end_y\"]].to_csv(save_path, index=False)\n","\n","print(\"Saved:\", save_path)"],"metadata":{"id":"c3_ULFUNMdYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lodFaGnFrHS-"},"execution_count":null,"outputs":[]}]}