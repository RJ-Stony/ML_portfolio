{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"gpuType":"T4","mount_file_id":"1RY0Fx5EBoe7SHdFMOWy05HSYgreAu28Q","authorship_tag":"ABX9TyNVf/OL+8Yxd2QJrlFloRvn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 0. 사전 세팅"],"metadata":{"id":"LrAsOz8GWlra"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGieDGDXVh4D"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","%cd \"/content/drive/MyDrive/데이터 분석/projects/ML_portfolio/10_kleague_final_pass_prediction\""]},{"cell_type":"code","source":["!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"],"metadata":{"id":"--aOM1riNtXk","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. EDA 인사이트"],"metadata":{"id":"JsNdGhiOy9Px"}},{"cell_type":"markdown","source":["---\n","\n","▸ 타겟 구조\n","\n","    에피소드별 마지막 이벤트는 항상 Pass이고, 그 Pass의 (end_x, end_y)가 타겟\n","    패스 성공/실패 비율 ≈ 56:44 정도로 아주 심하게 치우치지 않음\n","    거리 기준으로 보면 10~20m 구간의 패스 성공률이 가장 높고, 30m 이상부터 급격히 떨어짐\n","\n","    < 인사이트 >\n","    - 순수 좌표 회귀(MAE/MSE) + 거리 관련 feature를 같이 쓰면 좋을 듯 !\n","    - multi-task로 distance_bin(0–10 / 10–20 / ...)이나 zone까지 같이 예측하게 하면 representation quality가 좋아질 수 있음\n","\n","▸ 시퀀스/정렬 구조\n","\n","    전체 에피소드 중 98% 이상은 시간 순서 정렬이 깔끔하고, 나머지 1.8% 정도만 역전이 있음\n","    대부분 -0.1 이내의 미세 오차, 진짜 심각한 역전은 적음\n","\n","    < 인사이트 >\n","    - 시퀀스 모델은 써도 될 것 같고 !\n","    - 단, 전처리에서 time_seconds, action_id 기준으로 확실하게 정렬하고, 마스크 처리만 잘 해주면 됨\n","    - 몇 개 안 되는 진짜 이상치는 드롭하거나 별도 처리해도 전체 성능에 영향 거의 없음\n","\n","▸ 이벤트 구성 패턴\n","\n","    전체 이벤트 중 Pass ≈ 50%, Carry ≈ 23%, Turnover 관련(Recovery + Interception + Tackle + Duel) ≈ 15%\n","    최빈 bigram: Pass→Pass, Carry→Pass, Pass→Carry, Turnover→Pass\n","    최빈 trigram: Pass-Carry-Pass, Pass-Pass-Pass, Carry-Pass-Pass 등\n","\n","    < 인사이트 >\n","    - n-gram 구조가 강해서 TCN(1D conv), RNN, Transformer 모두 잘 맞는 도메인\n","    - 특히 Carry → Pass, Turnover → Pass 같은 패턴은 final pass 위치와 전술 의도를 암시해줄 수 있음\n","\n","▸ 마지막 이벤트 직전 패턴\n","\n","    Final pass 바로 직전 이벤트의 90%가 Carry / Pass / Recovery 셋 중 하나\n","    평균 end_x/거리 기준으로 보면 Carry / Tackle 직후 패스가 가장 전방, 가장 먼 거리\n","\n","    < 인사이트 >\n","    - prev_event_type, prev_event_dx/dy/angle는 무조건 써야하는 feature\n","    - 심지어 “마지막 3~4 스텝만 따로 뽑아서 쓰는 모델”도 하나의 strong baseline으로 가능할 듯\n","\n","▸ Episode별 클러스터 (5개 패턴)\n","\n","    Balanced build-up / 짧은 측면 전개 / 리셋/후퇴 패턴 / 짧은 반대 측면 전개 / 긴 빌드업 (dist_cum 가장 큼)\n","    마지막 패스 성공률까지 보면 Cluster 4 (Long Build-up)가 최상(~0.63)\n","\n","    < 인사이트 >\n","    - episode에 붙는 cluster_id 자체가 embedding으로 사용될 수 있음\n","    - Mixture-of-experts / cluster-wise head 같은 구조도 고려 가능\n","    - 최소한 cluster_id를 one-hot 또는 embedding으로 넣으면, “지금의 빌드업 흐름이 어떤 종류인지” 모델이 한 번에 인식\n","\n","▸ Player별 분석 결과\n","\n","    선수별 carry_ratio 분포가 그렇게 극단적이지 않음\n","    선수별 angle_mean, final pass (end_x, end_y) mean 위치도 좁은 범위에 몰려 있고, 뚜렷한 클러스터 구조 없음\n","\n","    < 인사이트 >\n","    - player_id embedding은 효과 대비 리스크(차원+노이즈) 가 큼\n","    - baseline에서는 아예 빼고 시작하는 게 합리적\n","    - 나중에 여유 있으면 작은 차원(8~16) + strong dropout으로 시험해보는 정도\n","\n","▸ Episode별 움직임 & 각도 smoothness\n","\n","    cum_dx, cum_dy로 episode가 전진 위주인지, 좌우 측면 전개인지가 드러남\n","    angle 변화량 기준으로 보면, 누적 전진량이 클수록 angle이 더 안정(episode가 한 방향으로 쭉 진행)\n","\n","    < 인사이트 >\n","    - cum_dx, cum_dy, movement_norm, angle_mean_abs, angle_std는 전술 패턴을 대표하는 episode별 feature\n","    - final pass의 zone/거리/각도 예측에 직접적인 신호를 줌\n","\n","▸ Turnover 이후 3-step window\n","\n","    Turnover 직후 첫 행동은 dx ~ 0 (잡아두기), 그 이후 2~3 step에서 전진/측면 전개가 본격적으로 나타남\n","\n","    < 인사이트 >\n","    - “turnover 이후 k-step” 여부를 표시하는 feature가 유용하게 쓰일 수 있음\n","    - 특히 final pass가 turnover 직후 짧은 시퀀스에서 나오는지, 긴 빌드업 끝에서 나오는지 구분해줄 수 있음"],"metadata":{"id":"T3sdgeqPzBv5"}},{"cell_type":"markdown","source":["# 8. EDA에 이은 Feature Engineering (함수 정의)"],"metadata":{"id":"U9v6nLtX4Kpq"}},{"cell_type":"markdown","source":["---\n","\n","▸ 패스 각도(angle)\n","\n","    angle = arctan((end_y-start_y) / (end_x-start_x))\n","\n","    풀백은 측면으로 많이 주고, 중앙 미드필더는 전진 패스의 비율 높음\n","    수비수는 옆으로 주는 패스나 후방 패스의 비중 높음\n","\n","▸ 패스 진행 거리\n","\n","    더 먼 패스일수록 progressive chance가 높고, end_x가 강하게 증가하는 패턴을 가짐\n","\n","▸ event_type 임베딩\n","\n","    type_name → embedding vector\n","    result_name → embedding vector\n","\n","    sequence embedding에 필수적으로 진행해야하는 것\n","\n","▸ 에피소드에서의 속도(Δx, Δy)\n","\n","    dx_t = x_t - x_(t-1)\n","    dy_t = y_t - y_(t-1)\n","\n","    엔드 투 엔드 모델보다 훨씬 패턴 학습이 잘 됨\n","\n","    dx > 0 → 오른쪽으로 전진 중\n","    dy > 0 → 위쪽으로 이동 중\n","    dy < 0 → 아래쪽으로 이동 중\n","    dx ≈ 0 → 횡패스 빈도 높음\n","    dx < 0 → 후방 패스 비율 증가 (안정화)\n","\n","    1. 한 에피소드에서 dx가 계속 증가한다 ➜ 공격 전개 중 (전진 패스 가능성이 높음)\n","    2. dy가 크게 증가했다➜ 측면 전개 중 (사이드로 패스가 날아갈 가능성)\n","    3. dx가 음수로 전환되었다 ➜ 후방 안정화 패스 패턴\n","    4. dx, dy가 급격히 바뀐다 ➜ 압박을 벗어나기 위한 빠른 전개"],"metadata":{"id":"-ZPcy6vA4OFP"}},{"cell_type":"markdown","source":["## 8.1 공통 Util & 기본 데이터 정렬"],"metadata":{"id":"HvP_0jkzmws9"}},{"cell_type":"markdown","source":["### 8.1.1 이벤트 단순화(클러스터링), Zone 함수 (EDA에서 쓰던 것 정리)"],"metadata":{"id":"zS22enxvm3PT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfpfRXNllmfT"},"outputs":[],"source":["from collections import Counter\n","from math import log2\n","\n","# 이벤트 타입 단순화하는 함수\n","def simplify_event(t: str) -> str:\n","    # Pass 계열\n","    if t in [\"Pass\", \"Pass_Freekick\", \"Pass_Corner\"]:\n","        return \"Pass\"\n","\n","    # Carry\n","    if t == \"Carry\":\n","        return \"Carry\"\n","\n","    # Duel / Turnover 계열\n","    if t in [\"Duel\", \"Tackle\", \"Interception\", \"Recovery\"]:\n","        return \"Duel_Turnover\"\n","\n","    # Cross (정확히 Cross만)\n","    if t == \"Cross\":\n","        return \"Cross\"\n","\n","    # Shot 계열\n","    if t.startswith(\"Shot\"):\n","        return \"Shot\"\n","\n","    # Penalty Kick은 Shot 계열로 통합\n","    if t == \"Penalty Kick\":\n","        return \"Shot\"\n","\n","    # Clearance\n","    if t in [\"Clearance\", \"Aerial Clearance\"]:\n","        return \"Clearance\"\n","\n","    # GK Action\n","    if t in [\"Catch\", \"Parry\", \"Goal Kick\", \"Keeper Rush-Out\"]:\n","        return \"GK_Action\"\n","\n","    # Block / Deflection / Intervention / Hit\n","    if t in [\"Block\", \"Deflection\", \"Intervention\", \"Hit\"]:\n","        return \"Deflect_Block\"\n","\n","    # Set-piece\n","    if t == \"Throw-In\":\n","        return \"SetPiece\"\n","\n","    # Goal 이벤트\n","    if t in [\"Goal\", \"Own Goal\"]:\n","        return \"Goal_Event\"\n","\n","    # Error 계열\n","    if t in [\"Error\", \"Out\", \"Foul\", \"Foul_Throw\", \"Handball_Foul\", \"Offside\"]:\n","        return \"Error_Out\"\n","\n","    return \"Misc\"\n","\n","# 이벤트 결과 단순화하는 함수\n","def simplify_result(result_name):\n","    if result_name in [\"Successful\", \"On Target\", \"Goal\"]:\n","        return \"Success\"\n","\n","    if result_name in [\"Unsuccessful\", \"Off Target\", \"Blocked\"]:\n","        return \"Fail\"\n","\n","    return \"None\"\n","\n","# Zone 구분하는 함수\n","def get_zone_x(x):\n","    if x < 35: return \"D3\"\n","    elif x < 70: return \"M3\"\n","    else: return \"A3\"\n","\n","def get_zone_y(y):\n","    if y < 22: return \"Left\"\n","    elif y < 45: return \"Center\"\n","    else: return \"Right\"\n","\n","# 시퀀스(에피소드) 엔트로피 측정하는 함수\n","def sequence_entropy(seq):\n","    cnt = Counter(seq)\n","    total = len(seq)\n","\n","    if total == 0:\n","        return 0.0\n","\n","    probs = [c / total for c in cnt.values()]\n","\n","    return -sum(p * log2(p) for p in probs if p > 0)"]},{"cell_type":"markdown","source":["### 8.1.2 기본 정렬 함수\n","\n","---\n","\n","    정렬이 이미 되어있는 데이터라 재정렬시키면 깨질 수 있음 - 삭제"],"metadata":{"id":"CnydgHlEqMkD"}},{"cell_type":"code","source":["# SORT_COLS = [\"game_episode\", \"time_seconds\", \"action_id\"]\n","\n","# def sort_events(df: pd.DataFrame) -> pd.DataFrame:\n","#     \"\"\"\n","#     time_seconds, action_id 기준으로 episode 내 이벤트 정렬.\n","#     \"\"\"\n","#     df_sorted = df.sort_values(SORT_COLS).reset_index(drop=True)\n","#     return df_sorted"],"metadata":{"id":"ZPemOp1FolOF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8.2 이벤트별 Feature Engineering"],"metadata":{"id":"E3IZ85NIqT1I"}},{"cell_type":"markdown","source":["---\n","\n","    한 이벤트마다 어떤 Feature를 만들지를 담당하는 함수"],"metadata":{"id":"RyzkRhA2qbrG"}},{"cell_type":"markdown","source":["### 8.2.1 Turnover flag 계산 (EDA에서 쓴 함수)"],"metadata":{"id":"O2nl_EnMu3Kf"}},{"cell_type":"code","source":["def add_turnover_flag(df):\n","    df = df.copy()\n","\n","    # Fail 정의\n","    fail = df[\"result_simple\"] == \"Fail\"\n","\n","    # Pass / Cross / SetPiece 실패 → turnover\n","    cond_fail_pass = df[\"event_simple\"].isin([\"Pass\", \"Cross\", \"SetPiece\"]) & fail\n","\n","    # Take-On 실패\n","    cond_takeon_fail = (df[\"type_name\"] == \"Take-On\") & (df[\"result_name\"] == \"Unsuccessful\")\n","\n","    # Duel 실패\n","    cond_duel_fail = (df[\"type_name\"] == \"Duel\") & (df[\"result_name\"] == \"Unsuccessful\")\n","\n","    # 상대가 소유권 획득하는 이벤트\n","    cond_gain = df[\"event_simple\"] == \"Duel_Turnover\"\n","\n","    # Dead ball turnover\n","    cond_deadball = df[\"event_simple\"] == \"Error_Out\"\n","\n","    df[\"is_turnover\"] = (\n","        cond_fail_pass |\n","        cond_takeon_fail |\n","        cond_duel_fail |\n","        cond_gain |\n","        cond_deadball\n","    ).astype(int)\n","\n","    return df"],"metadata":{"id":"fg2Te6kwqSvz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.2.2 episode 내 좌표 차이 / 시간 차이 등 계산 함수"],"metadata":{"id":"VEHnIAg6xCRW"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","def add_movement_features(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    episode 내 start_x, start_y 기준으로 dx, dy, distance, angle, dt 등 추가.\n","    \"\"\"\n","    df = df.copy()\n","    # df = sort_events(df)\n","\n","    df[\"dx\"] = df.groupby(\"game_episode\")[\"start_x\"].diff().fillna(0)\n","    df[\"dy\"] = df.groupby(\"game_episode\")[\"start_y\"].diff().fillna(0)\n","\n","    df[\"distance\"] = np.sqrt(df[\"dx\"]**2 + df[\"dy\"]**2)\n","\n","    df[\"angle\"] = np.arctan2(df[\"dy\"], df[\"dx\"]).fillna(0)\n","\n","    # 시간차 안정화\n","    dt = df.groupby(\"game_episode\")[\"time_seconds\"].diff()\n","    dt = dt.fillna(0)\n","    dt[dt < 0] = 0\n","    df[\"dt\"] = dt\n","\n","    # step index\n","    df[\"step_idx\"] = df.groupby(\"game_episode\").cumcount()\n","    df[\"epi_len\"] = df.groupby(\"game_episode\")[\"step_idx\"].transform(\"max\") + 1\n","\n","    df[\"step_idx_norm\"] = df[\"step_idx\"] / df[\"epi_len\"].clip(lower=1)\n","\n","    # relative time\n","    t_min = df.groupby(\"game_episode\")[\"time_seconds\"].transform(\"min\")\n","    t_max = df.groupby(\"game_episode\")[\"time_seconds\"].transform(\"max\")\n","    df[\"time_rel\"] = (df[\"time_seconds\"] - t_min) / (t_max - t_min).replace(0, 1)\n","\n","    return df"],"metadata":{"id":"axknzD2vw2Ux"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.2.3 zone / 골 방향 feature 함수"],"metadata":{"id":"KpGk9YfxyBG2"}},{"cell_type":"code","source":["def add_categorical_features(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","\n","    # 단순화\n","    df[\"event_simple\"] = df[\"type_name\"].apply(simplify_event)\n","    df[\"result_simple\"] = df[\"result_name\"].apply(simplify_result)\n","\n","    # zone\n","    df[\"zone_x\"] = df[\"start_x\"].apply(get_zone_x)\n","    df[\"zone_y\"] = df[\"start_y\"].apply(get_zone_y)\n","\n","    # 골대 기준 거리/각도 (오른쪽 골대 기준)\n","    goal_x, goal_y = 105.0, 34.0\n","    df[\"dist_to_goal\"] = np.sqrt((goal_x - df[\"start_x\"])**2 +\n","                                 (goal_y - df[\"start_y\"])**2)\n","    goal_angle = np.arctan2(goal_y - df[\"start_y\"],\n","                            goal_x - df[\"start_x\"])\n","    df[\"angle_to_goal\"] = goal_angle\n","\n","    return df"],"metadata":{"id":"bx2dbhxsxM5U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.2.4 Episode 누적 이동량 계산 함수"],"metadata":{"id":"7yFTd4zZyMbs"}},{"cell_type":"code","source":["def add_episode_cumulative_movement(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    # df = sort_events(df)\n","\n","    df[\"cum_dx\"] = df.groupby(\"game_episode\")[\"dx\"].cumsum()\n","    df[\"cum_dy\"] = df.groupby(\"game_episode\")[\"dy\"].cumsum()\n","    df[\"movement_norm\"] = np.sqrt(df[\"cum_dx\"]**2 + df[\"cum_dy\"]**2)\n","\n","    return df"],"metadata":{"id":"M0HvCTVSyHUK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.2.5 최종 적용 함수"],"metadata":{"id":"ZIH77OvoyR3T"}},{"cell_type":"code","source":["def build_event_level_features(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    train_df나 test_episode_df에 공통 적용할 Event-level Feature Engineering 파이프라인\n","    \"\"\"\n","    df_fe = df.copy()\n","    # df_fe = sort_events(df_fe)\n","    df_fe = add_categorical_features(df_fe)\n","    df_fe = add_turnover_flag(df_fe)\n","    df_fe = add_movement_features(df_fe)\n","    df_fe = add_episode_cumulative_movement(df_fe)\n","\n","    return df_fe"],"metadata":{"id":"52U0CLh-yT35"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8.3 에피소드별 Feature Engineering"],"metadata":{"id":"8F9ruFF_ym9u"}},{"cell_type":"markdown","source":["### 8.3.1 각도 변화량 요약 함수"],"metadata":{"id":"H5ByjDn8yruM"}},{"cell_type":"code","source":["def angle_diff(a1, a2):\n","    diff = a2 - a1\n","    diff = (diff + np.pi) % (2 * np.pi) - np.pi\n","    return diff\n","\n","def compute_angle_smoothness(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    episode별 angle 변화량 요약 (std, mean_abs, max 등)\n","    \"\"\"\n","    df = df.copy()\n","    # df = sort_events(df)\n","\n","    records = []\n","\n","    for ge, g in df.groupby(\"game_episode\"):\n","        ang = g[\"angle\"].values\n","        if len(ang) < 3:\n","            continue\n","\n","        diffs = [angle_diff(ang[i], ang[i+1]) for i in range(len(ang) - 1)]\n","\n","        records.append({\n","            \"game_episode\": ge,\n","            \"angle_change_std\": np.std(diffs),\n","            \"angle_change_mean_abs\": np.mean(np.abs(diffs)),\n","            \"angle_change_max\": np.max(np.abs(diffs)),\n","            \"angle_change_N\": len(diffs),\n","        })\n","\n","    angle_df = pd.DataFrame(records)\n","    return angle_df\n","\n","def add_angle_smoothness_to_epi(epi_feat: pd.DataFrame,\n","                                angle_smooth_df: pd.DataFrame) -> pd.DataFrame:\n","\n","    res = epi_feat.merge(angle_smooth_df, on=\"game_episode\", how=\"left\")\n","    # 결측은 0 또는 평균값으로 채워도 됨 (길이가 짧은 에피소드)\n","    res[[\"angle_change_std\", \"angle_change_mean_abs\",\n","         \"angle_change_max\", \"angle_change_N\"]] = \\\n","        res[[\"angle_change_std\", \"angle_change_mean_abs\",\n","             \"angle_change_max\", \"angle_change_N\"]].fillna(0.0)\n","\n","    return res"],"metadata":{"id":"rId_Lb7pyaxO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.3.2 episode별 요약 함수"],"metadata":{"id":"PJvIXbagy1cr"}},{"cell_type":"code","source":["def extract_episode_summary(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    episode별 요약 feature (len, ratio_pass/carry, dx/dy, dist 등)\n","    \"\"\"\n","    df = df.copy()\n","    # df = sort_events(df)\n","\n","    feats = []\n","\n","    for ge, g in df.groupby(\"game_episode\"):\n","        event_s = g[\"event_simple\"].values\n","\n","        xs = g[\"start_x\"].values\n","        ys = g[\"start_y\"].values\n","\n","        dx = np.diff(xs)\n","        dy = np.diff(ys)\n","        dist = np.sqrt(dx*dx + dy*dy)\n","        angle = np.arctan2(dy, dx)\n","\n","        len_epi = len(g)\n","\n","        feats.append({\n","            \"game_episode\": ge,\n","            \"epi_len\": len_epi,\n","\n","            # 단순화 버전 적용해서\n","            \"ratio_pass\": np.mean(event_s == \"Pass\"),\n","            \"ratio_carry\": np.mean(event_s == \"Carry\"),\n","            \"ratio_turnover\": np.mean(g[\"is_turnover\"].values),\n","\n","            \"dx_mean\": dx.mean() if len(dx) else 0,\n","            \"dy_mean\": dy.mean() if len(dy) else 0,\n","            \"angle_mean\": angle.mean() if len(angle) else 0,\n","            \"angle_std\": angle.std() if len(angle) else 0,\n","\n","            \"dist_mean\": dist.mean() if len(dist) else 0,\n","            \"dist_cum\": dist.sum() if len(dist) else 0,\n","\n","            \"start_zone_x\": get_zone_x(xs[0]),\n","            \"start_zone_y\": get_zone_y(ys[0]),\n","        })\n","\n","    return pd.DataFrame(feats)"],"metadata":{"id":"KR0tHHYWy4a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.3.3 episode별 event entropy 추가 함수"],"metadata":{"id":"fuYdzWjBzQhU"}},{"cell_type":"code","source":["def add_episode_entropy(df: pd.DataFrame, epi_feat: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    episode별 event_simplified entropy 계산 후 epi_feat에 merge\n","    \"\"\"\n","    df = df.copy()\n","    # df = sort_events(df)\n","\n","    entropy_records = []\n","    for ge, g in df.groupby(\"game_episode\"):\n","        seq = g[\"event_simple\"].tolist()\n","        ent = sequence_entropy(seq)\n","        entropy_records.append({\"game_episode\": ge, \"entropy_event\": ent})\n","\n","    ent_df = pd.DataFrame(entropy_records)\n","\n","    epi_feat = epi_feat.merge(ent_df, on=\"game_episode\", how=\"left\")\n","    return epi_feat"],"metadata":{"id":"zs0QT95azTu-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.3.4 Episode summary 통합 최종 함수"],"metadata":{"id":"-jguJq9ZzjBM"}},{"cell_type":"code","source":["def build_episode_level_features(df_fe: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Event별 FE가 적용된 df_fe를 입력 받아,\n","    episode별 summary feature를 생성.\n","    \"\"\"\n","    epi_feat = extract_episode_summary(df_fe)\n","    angle_smooth_df = compute_angle_smoothness(df_fe)\n","    epi_feat = add_angle_smoothness_to_epi(epi_feat, angle_smooth_df)\n","    epi_feat = add_episode_entropy(df_fe, epi_feat)\n","\n","    return epi_feat"],"metadata":{"id":"owLL4Lv8zlEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","    일단 Baseline 모델에는\n","    Event-level에서 event_simple (embedding), result_simple (embedding), is_turnover, dx, dy, distance,\n","    angle, dt, zone_x/y (embedding), step_idx_norm, time_rel, cum_dx, cum_dy, movement_norm을 Input으로 넣고,\n","\n","    Episode-level에서는 epi_len, ratio_pass, ratio_carry, ratio_turnover, dx_mean, dy_mean,\n","    angle_mean, angle_std, dist_cum, dist_mean, angle_smoothness metrics, entropy_event을 Input으로 넣을 듯 !\n"],"metadata":{"id":"oxhQPLZQ4HTS"}},{"cell_type":"markdown","source":["# 9. 실제 Feature Engineering"],"metadata":{"id":"T2JgjWD05aJC"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('Data/train.csv')"],"metadata":{"id":"qFHbPz7c5edz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_fe = build_event_level_features(df)\n","df_fe.head()"],"metadata":{"id":"EKs2ZxGFz8tG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_fe.info()"],"metadata":{"id":"0sTMCB7h6pri"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 356721 entries, 0 to 356720\n","Data columns (total 34 columns):\n"," #   Column         Non-Null Count   Dtype  \n","---  ------         --------------   -----  \n"," 0   game_id        356721 non-null  int64  \n"," 1   period_id      356721 non-null  int64  \n"," 2   episode_id     356721 non-null  int64  \n"," 3   time_seconds   356721 non-null  float64\n"," 4   team_id        356721 non-null  int64  \n"," 5   player_id      356721 non-null  int64  \n"," 6   action_id      356721 non-null  int64  \n"," 7   type_name      356721 non-null  object\n"," 8   result_name    216467 non-null  object\n"," 9   start_x        356721 non-null  float64\n"," 10  start_y        356721 non-null  float64\n"," 11  end_x          356721 non-null  float64\n"," 12  end_y          356721 non-null  float64\n"," 13  is_home        356721 non-null  bool   \n"," 14  game_episode   356721 non-null  object\n"," 15  event_simple   356721 non-null  object\n"," 16  result_simple  356721 non-null  object\n"," 17  zone_x         356721 non-null  object\n"," 18  zone_y         356721 non-null  object\n"," 19  dist_to_goal   356721 non-null  float64\n"," 20  angle_to_goal  356721 non-null  float64\n"," 21  is_turnover    356721 non-null  int64  \n"," 22  dx             356721 non-null  float64\n"," 23  dy             356721 non-null  float64\n"," 24  distance       356721 non-null  float64\n"," 25  angle          356721 non-null  float64\n"," 26  dt             356721 non-null  float64\n"," 27  step_idx       356721 non-null  int64  \n"," 28  epi_len        356721 non-null  int64  \n"," 29  step_idx_norm  356721 non-null  float64\n"," 30  time_rel       356721 non-null  float64\n"," 31  cum_dx         356721 non-null  float64\n"," 32  cum_dy         356721 non-null  float64\n"," 33  movement_norm  356721 non-null  float64\n","dtypes: bool(1), float64(17), int64(9), object(7)\n","memory usage: 90.2+ MB\n","```\n","\n"],"metadata":{"id":"jb89ftNg6siD"}},{"cell_type":"code","source":["df_fe[['type_name', 'result_name', 'event_simple', 'result_simple']]"],"metadata":{"id":"2blRmL2q6Xsy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epi_fe = build_episode_level_features(df_fe)\n","epi_fe.head()"],"metadata":{"id":"za7QG0OC5ie2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epi_fe.info()"],"metadata":{"id":"b_EGa6XJ7FEO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 15435 entries, 0 to 15434\n","Data columns (total 18 columns):\n"," #   Column                 Non-Null Count  Dtype  \n","---  ------                 --------------  -----  \n"," 0   game_episode           15435 non-null  object\n"," 1   epi_len                15435 non-null  int64  \n"," 2   ratio_pass             15435 non-null  float64\n"," 3   ratio_carry            15435 non-null  float64\n"," 4   ratio_turnover         15435 non-null  float64\n"," 5   dx_mean                15435 non-null  float64\n"," 6   dy_mean                15435 non-null  float64\n"," 7   angle_mean             15435 non-null  float64\n"," 8   angle_std              15435 non-null  float64\n"," 9   dist_mean              15435 non-null  float64\n"," 10  dist_cum               15435 non-null  float64\n"," 11  start_zone_x           15435 non-null  object\n"," 12  start_zone_y           15435 non-null  object\n"," 13  angle_change_std       15435 non-null  float64\n"," 14  angle_change_mean_abs  15435 non-null  float64\n"," 15  angle_change_max       15435 non-null  float64\n"," 16  angle_change_N         15435 non-null  float64\n"," 17  entropy_event          15435 non-null  float64\n","dtypes: float64(14), int64(1), object(3)\n","memory usage: 2.1+ MB\n","```\n","\n"],"metadata":{"id":"wWFFtweA7RSf"}},{"cell_type":"code","source":["epi_fe.describe().T.round(2)"],"metadata":{"id":"i2ph1hm77PyM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9.1 train_fe 파일 저장"],"metadata":{"id":"V9hJ-Y7lcJ8Q"}},{"cell_type":"code","source":["df_train = pd.read_csv(\"Data/train.csv\")\n","df_fe = build_event_level_features(df_train)\n","epi_fe = build_episode_level_features(df_fe)\n","\n","# df_fe.to_csv(\"Data/train_fe.csv\", index=False)\n","# epi_fe.to_csv(\"Data/train_epi_fe.csv\", index=False)"],"metadata":{"id":"we7AvDRBbk2_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9.2 test_fe 파일 저장"],"metadata":{"id":"b1CnC9HgcNMl"}},{"cell_type":"code","source":["df_test = pd.read_csv('Data/test.csv')\n","df_test.head()"],"metadata":{"id":"8Y74N21QcP-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from tqdm import tqdm\n","from joblib import Parallel, delayed\n","\n","def load_test_episode_opt(path, game_id, game_episode, base_dir=\"Data\"):\n","    fname = path[1:]\n","    file_path = base_dir + fname\n","\n","    df = pd.read_csv(file_path)\n","    df[\"game_id\"] = game_id\n","    df[\"game_episode\"] = game_episode\n","\n","    return df\n","\n","test_meta = pd.read_csv(\"Data/test.csv\")\n","\n","test_events = Parallel(n_jobs=-1)(\n","    delayed(load_test_episode_opt)(p, g, e)\n","    for p, g, e in tqdm(zip(test_meta[\"path\"], test_meta[\"game_id\"], test_meta[\"game_episode\"]), total=len(test_meta))\n",")\n","\n","test_df = pd.concat(test_events, ignore_index=True)\n","\n","test_fe = build_event_level_features(test_df)\n","test_epi_fe = build_episode_level_features(test_fe)\n","\n","test_fe.to_csv(\"Data/test_fe.csv\", index=False)\n","test_epi_fe.to_csv(\"Data/test_epi_fe.csv\", index=False)"],"metadata":{"id":"-EkKRnmLcVmN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. 모델링"],"metadata":{"id":"trOzMMR88UuQ"}},{"cell_type":"markdown","source":["## 10.1 train/valid split"],"metadata":{"id":"X9aRKGEi8W91"}},{"cell_type":"markdown","source":["---\n","\n","    에피소드 단위로 split해야겠다 !\n","\n","    시퀀스 모델은 episode 전체를 하나의 샘플로 보고 학습하기 때문에 episode를 반으로 쪼개거나 섞으면 temporal dependency가 깨짐\n","\n","    같은 game_id 안에서 train/valid가 섞이면 데이터 누수 발생\n","    따라서 game_id 단위로 묶어서 episode 단위 split하는 게 가장 안전\n","\n","    그 중에서도 Game별로 Split 해야할 것 같은데,\n","    train 게임과 valid 게임을 완전히 분리하고, 하나의 game_id에 속한 모든 episode는 train 또는 valid 중 하나에만 배정하기\n","\n","    데이터 누수 0이고, 가장 현실적이라고 판단되기 때문에 이 기법으로 선택"],"metadata":{"id":"eFWSOk1W8aXw"}},{"cell_type":"code","source":["df = pd.read_csv('Data/train.csv')\n","\n","# 나누기 전에 tail 검증부터 (모두 최종 패스가 나와야 함)\n","orig_tail = df.groupby(\"game_episode\").tail(1)\n","\n","orig_tail_types = orig_tail[\"type_name\"].unique()\n","print(orig_tail_types)"],"metadata":{"id":"ZUqTKyXHBqQl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","['Pass']\n","```\n","\n"],"metadata":{"id":"dPQlzcvcB6Gu"}},{"cell_type":"code","source":["fe_tail = df_fe.groupby(\"game_episode\").tail(1)\n","fe_tail_types = fe_tail[\"type_name\"].unique()\n","\n","print(fe_tail_types)"],"metadata":{"id":"UrNnxgls7YID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","['Pass' 'Carry']가 나온 걸 보아하니.. FE 과정에서 문제가 생긴 것 같아 뜯어고쳐봐야겠다\n","\n","⭐ 재정렬을 시키면 안 됐었다 !! 정렬 함수 삭제하니 ['Pass']만 나온다.\n","```\n","\n"],"metadata":{"id":"B0sYQxcSB9uj"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","'''\n","게임 단위 split (데이터 누수 방지)\n","'''\n","game_ids = df[\"game_id\"].unique()\n","\n","train_games, valid_games = train_test_split(\n","    game_ids,\n","    test_size=0.2,\n","    random_state=42,\n",")\n","\n","'''\n","에피소드 단위 split 기준 만들기 (split 기준은 반드시 원본 df에서 뽑기)\n","'''\n","train_epis = df[df[\"game_id\"].isin(train_games)][\"game_episode\"].unique()\n","valid_epis = df[df[\"game_id\"].isin(valid_games)][\"game_episode\"].unique()\n","\n","print(f\"Train games: {len(train_games)}, Valid games: {len(valid_games)}\")\n","print(f\"Train episodes: {len(train_epis)}, Valid episodes: {len(valid_epis)}\")\n","\n","'''\n","FE 이후 df_fe에서 에피소드 기준으로 데이터 분리\n","'''\n","train_df = df_fe[df_fe[\"game_episode\"].isin(train_epis)].copy()\n","valid_df = df_fe[df_fe[\"game_episode\"].isin(valid_epis)].copy()\n","\n","'''\n","Episode tail 검증\n","'''\n","train_tail_types = train_df.groupby(\"game_episode\").tail(1)[\"type_name\"].unique()\n","valid_tail_types = valid_df.groupby(\"game_episode\").tail(1)[\"type_name\"].unique()\n","\n","print(\"Train tail types:\", train_tail_types)\n","print(\"Valid tail types:\", valid_tail_types)\n","\n","assert set(train_tail_types) == {\"Pass\"}\n","assert set(valid_tail_types) == {\"Pass\"}\n","\n","print(\"Split integrity confirmed: all episode tails are Pass.\")"],"metadata":{"id":"NdJNpFyOAuaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","Train games: 158, Valid games: 40\n","Train episodes: 12389, Valid episodes: 3046\n","Train tail types: ['Pass']\n","Valid tail types: ['Pass']\n","Split integrity confirmed: all episode tails are Pass.\n","```\n","\n"],"metadata":{"id":"2FqRfHj_Ef4C"}},{"cell_type":"code","source":["print(len(train_epis), len(valid_epis))\n","print(train_df.shape, valid_df.shape)"],"metadata":{"id":"PUx26aZD_wCP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["    12389 3046\n","    (285011, 34) (71710, 34)"],"metadata":{"id":"hmX-gVQA_3EI"}},{"cell_type":"markdown","source":["## 10.2 모델 입력 구조 설계"],"metadata":{"id":"mJSdAx2LFgoI"}},{"cell_type":"markdown","source":["---\n","\n","    FE(df_fe)는 “이벤트 1개 = 1 row” 형태고, 모델은 “episode 전체 = 1 sample(sequence)” 형태를 원함\n","\n","    최종적으로 각 episode는 다음과 같은 tensor로 구성\n","    - X_seq: 이벤트 시퀀스 feature (T × F)\n","    - mask: padding mask (T)\n","    - target_x, target_y: 에피소드 마지막 패스 end_x, end_y\n","    - (선택) categorical embedding index들\n","    - (선택) episode별 feature\n","\n","    일단 baseline은 event-level numeric features만 들어가는 baseline version으로 구성하고,\n","    categorical embedding은 이후 단계에서 추가해볼 버전에 넣어보든가 하기"],"metadata":{"id":"Qu24BRDgFiC5"}},{"cell_type":"markdown","source":["    Baseline Input Features\n","\n","| Feature        | 의미                   |\n","| -------------- | -------------------- |\n","| start_x, start_y         | 시작 좌표                  |\n","| dx, dy         | 이동량                  |\n","| distance       | 이동 거리                |\n","| angle          | 이동 방향                |\n","| dt             | 이벤트 간 시간차            |\n","| step_idx_norm  | 시퀀스 내 포지션            |\n","| time_rel       | 상대 시간                |\n","| cum_dx, cum_dy | 에피소드 누적 이동량          |\n","| movement_norm  | 누적 이동량 크기            |\n","| dist_to_goal   | 골대까지 거리              |\n","| angle_to_goal  | 골대 방향 각도             |\n","| is_turnover    | 1-step turnover flag |"],"metadata":{"id":"utkMF4T9IZ33"}},{"cell_type":"markdown","source":["### 10.2.1 입력 구조 설계 함수 정의"],"metadata":{"id":"ythJiZdAI40S"}},{"cell_type":"code","source":["def build_episode_sequences(df):\n","    \"\"\"\n","    df_fe를 episode 단위로 list로 묶어주는 함수.\n","    output: {game_episode: df_subset}\n","    \"\"\"\n","    episodes = {}\n","\n","    for ge, g in df.groupby(\"game_episode\"):\n","        g_sorted = g.sort_values([\"time_seconds\", \"action_id\"])\n","        episodes[ge] = g_sorted.reset_index(drop=True)\n","\n","    return episodes\n","\n","def extract_targets(episodes):\n","    targets = {}\n","\n","    for ge, g in episodes.items():\n","        last = g.iloc[-1]\n","        targets[ge] = (last[\"end_x\"], last[\"end_y\"])\n","\n","    return targets\n","\n","# 이벤트 레벨 Numeric Feature\n","CONT_COLS = [\n","    \"start_x\", \"start_y\",\n","    \"dx\", \"dy\",\n","    \"distance\",\n","    \"angle\",\n","    \"dt\",\n","    \"step_idx_norm\",\n","    \"time_rel\",\n","    \"cum_dx\", \"cum_dy\",\n","    \"movement_norm\",\n","    \"dist_to_goal\",\n","    \"angle_to_goal\",\n","    \"is_turnover\"\n","]\n","\n","def episode_to_matrix(g, feature_cols=CONT_COLS):\n","    \"\"\"\n","    한 episode의 df(여러 row) → Numeric Feature Matrix (T × F)\n","    \"\"\"\n","    return g[feature_cols].values.astype(\"float32\")"],"metadata":{"id":"Lb4bQ01UAGii"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.2.2 Padding + Attention Mask 생성"],"metadata":{"id":"SFzjTY5eJhRx"}},{"cell_type":"code","source":["def pad_sequence(seq, max_len):\n","    \"\"\"\n","    seq: (T, F)\n","    return:\n","      padded_seq: (max_len, F)\n","      mask: (max_len,)  — 1: 실제 token, 0: padding\n","    \"\"\"\n","    T, F = seq.shape\n","    pad_len = max_len - T\n","\n","    # 길이가 짧을 때, 부족한 만큼 0으로 Zero-padding\n","    # 이때 mask를 만들어 어디까지가 진짜 데이터이고, 어디부터가 0인지 표시(1은 데이터, 0은 패딩)\n","    if pad_len > 0:\n","        pad = np.zeros((pad_len, F), dtype=\"float32\")\n","        padded = np.concatenate([seq, pad], axis=0)\n","        mask = np.concatenate([np.ones(T), np.zeros(pad_len)])\n","    # 길이가 길 때, max_len만큼만 자르기\n","    else:\n","        padded = seq[:max_len]\n","        mask = np.ones(max_len)\n","\n","    return padded, mask.astype(\"float32\")"],"metadata":{"id":"gknaS56_Jgu9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.2.3 EpisodeDataset (Baseline: Numeric Only)"],"metadata":{"id":"758hDWN-Jyx3"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","class EpisodeDataset(Dataset):\n","    def __init__(self, df, episode_ids, max_len=270, feature_cols=CONT_COLS):\n","        \"\"\"\n","        df: train_fe DataFrame\n","        episode_ids: 학습 또는 검증에 사용할 episode list\n","        \"\"\"\n","        self.episodes = []\n","        self.max_len = max_len\n","        self.feature_cols = feature_cols\n","\n","        # Episode별 분리\n","        for ge, g in df[df[\"game_episode\"].isin(episode_ids)].groupby(\"game_episode\"):\n","            g = g.sort_values([\"time_seconds\", \"action_id\"]).reset_index(drop=True)\n","\n","            seq = episode_to_matrix(g, feature_cols=feature_cols)\n","            seq_pad, mask = pad_sequence(seq, max_len)\n","\n","            # Target: 마지막 패스 end_x, end_y\n","            tx, ty = g[\"end_x\"].iloc[-1], g[\"end_y\"].iloc[-1]\n","            target = np.array([tx, ty], dtype=\"float32\")\n","\n","            self.episodes.append((seq_pad, mask, target))\n","\n","    def __len__(self):\n","        return len(self.episodes)\n","\n","    def __getitem__(self, idx):\n","        x, mask, target = self.episodes[idx]\n","        return {\n","            \"x\": torch.tensor(x),\n","            \"mask\": torch.tensor(mask),\n","            \"target\": torch.tensor(target),\n","        }"],"metadata":{"id":"GEA64CVeJolL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.2.4 Collate 함수"],"metadata":{"id":"TIcPc86JjGVW"}},{"cell_type":"code","source":["def collate_fn(batch):\n","    x_list = [item[\"x\"] for item in batch]\n","    mask_list = [item[\"mask\"] for item in batch]\n","    y_list = [item[\"target\"] for item in batch]\n","\n","    x = torch.stack(x_list)\n","    mask = torch.stack(mask_list)\n","    y = torch.stack(y_list)\n","\n","    return x, mask, y"],"metadata":{"id":"x9lz4AmsjIlW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10.3 BiLSTM 모델 (Only Numeric)"],"metadata":{"id":"v1HLUYW8L4jI"}},{"cell_type":"markdown","source":["---\n","\n","    RNN 계열은 시퀀스 데이터 처리의 기본 골격 / 기존 연구들에서도 경기 이벤트, 스포츠 시계열 데이터에 LSTM / BiLSTM 사용 사례 많음\n","\n","    특히 양방향 BiLSTM은 앞뒤 문맥 모두 고려 가능, 빌드업 전체 흐름을 학습하기에 적합. 실제로 최근 축구 이벤트 기반 분석에서도 활용된 사례 존재\n","\n","[활용 사례](https://www.mdpi.com/2079-9292/13/20/4105?utm_source=chatgpt.com)\n","\n","    Transformer나 복잡한 구조는 이후 확장 후보로 두고, 먼저 “단순 + 안정 + 빠른 실험”을 위해 BiLSTM이 이상적"],"metadata":{"id":"mhv1ymF2Pgp8"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","class BiLSTMRegressor(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=128, num_layers=1, dropout=0.2):\n","        super().__init__()\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            bidirectional=True,\n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(hidden_dim * 2, 128),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(128, 2)  # end_x, end_y\n","        )\n","\n","    def forward(self, x, mask):\n","        \"\"\"\n","        x: (B, T, F)\n","        mask: (B, T) – 여기서는 사용하지 않지만 확장 가능\n","        \"\"\"\n","        lengths = mask.sum(dim=1).long()\n","        lengths_sorted, sort_idx = lengths.sort(descending=True)\n","\n","        x_sorted = x[sort_idx]\n","\n","        packed = pack_padded_sequence(\n","            x_sorted,\n","            lengths_sorted.cpu(),\n","            batch_first=True,\n","            enforce_sorted=True\n","        )\n","\n","        _, (h_n, _) = self.lstm(packed)\n","\n","        # Forward, Backward concat\n","        h_fwd = h_n[-2]\n","        h_bwd = h_n[-1]\n","        h = torch.cat([h_fwd, h_bwd], dim=-1)\n","\n","        # 원래 배치 순서로 되돌리기\n","        _, inv_idx = sort_idx.sort()\n","        h = h[inv_idx]\n","\n","        out = self.fc(h)\n","        return out"],"metadata":{"id":"Pf6XyX4BRqyu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["    test.csv에서도 동일한 encoder를 사용해야 하므로, encoders는 pickle로 저장해두고 inference에서 다시 load해야 함"],"metadata":{"id":"WJ9V80bbR3Nv"}},{"cell_type":"markdown","source":["### 10.3.1 Dataset 생성 & DataLoader 구성"],"metadata":{"id":"BpBo2LnISYHa"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_dataset = EpisodeDataset(df_fe, train_epis, max_len=270)\n","valid_dataset = EpisodeDataset(df_fe, valid_epis, max_len=270)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"],"metadata":{"id":"_KoGF8y-RwyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.3.2 Train Loop"],"metadata":{"id":"cwheClg5Sai1"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","\n","model = BiLSTMRegressor(input_dim=len(CONT_COLS)).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()\n","\n","def euclidean(pred, target):\n","    return torch.sqrt(((pred - target)**2).sum(dim=1)).mean().item()\n","\n","EPOCHS = 20\n","\n","for epoch in range(EPOCHS):\n","    # ----------------------- Train -----------------------\n","    model.train()\n","    train_loss = 0\n","\n","    for x, mask, y in train_loader:\n","        x, mask, y = x.to(device), mask.to(device), y.to(device)\n","\n","        pred = model(x, mask)\n","        loss = criterion(pred, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    # ----------------------- Valid -----------------------\n","    model.eval()\n","    val_loss = 0\n","    val_dist = 0\n","\n","    with torch.no_grad():\n","        for x, mask, y in valid_loader:\n","            x, mask, y = x.to(device), mask.to(device), y.to(device)\n","\n","            pred = model(x, mask)\n","            loss = criterion(pred, y)\n","\n","            val_loss += loss.item()\n","            val_dist += euclidean(pred, y)\n","\n","    print(f\"[Epoch {epoch}] \"\n","          f\"TrainLoss={train_loss:.4f} | \"\n","          f\"ValidLoss={val_loss:.4f} | \"\n","          f\"Euclidean={val_dist:.4f}\")\n","\n","torch.save(model.state_dict(), \"model.pt\")"],"metadata":{"id":"K1wHR7OqSKy8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","[Epoch 11] TrainLoss=80387.3156 | ValidLoss=18447.2292 | Euclidean=1604.0103\n","[Epoch 12] TrainLoss=80371.7264 | ValidLoss=18295.6875 | Euclidean=1599.7318\n","[Epoch 13] TrainLoss=79303.6462 | ValidLoss=19201.0154 | Euclidean=1617.4109\n","[Epoch 14] TrainLoss=79285.7698 | ValidLoss=18494.2226 | Euclidean=1570.6561 ⭐\n","[Epoch 15] TrainLoss=78982.6663 | ValidLoss=18193.1013 | Euclidean=1585.8891\n","[Epoch 16] TrainLoss=78339.7959 | ValidLoss=18259.0561 | Euclidean=1594.4622\n","[Epoch 17] TrainLoss=78466.6491 | ValidLoss=18590.7915 | Euclidean=1601.3634\n","[Epoch 18] TrainLoss=77232.0764 | ValidLoss=18761.6560 | Euclidean=1599.2085\n","[Epoch 19] TrainLoss=78248.9732 | ValidLoss=18124.3916 | Euclidean=1578.8435\n","```\n","    모델 성능이 매우 안 좋은 것을 확인\n","\n","    Categorical context(이벤트 타입, zone 등)가 빠져 있어서, 정보가 크게 손실된 상태고,\n","    Padding mask는 있지만 attention mechanism이 없음 !! LSTM이 front-loaded됨\n","\n","    Positional Encoding(better time embedding)도 없음\n","\n","    따라서 아래 절차처럼 한 번 모델 하나하나씩 구현해보자\n","\n","    1. Categorical Embedding 추가 ⭐\n","    2. Episode-level Feature 추가\n","    3. BiLSTM → BiLSTM + Attention(Luong/Scaled Dot) 업그레이드\n","    4. Transformer Baseline 추가\n","    5. multi-task 학습: zone_x_bin, zone_y_bin 또한 같이 예측\n","    6. 좌표 normalization 도입\n","    7. loss 개선: MAE + custom distance loss 혼합\n"],"metadata":{"id":"HTBFG-ZAmIS8"}},{"cell_type":"markdown","source":["### 10.3.3 Inference"],"metadata":{"id":"46cuTapBS2Ms"}},{"cell_type":"code","source":["df_test_fe = pd.read_csv(\"Data/test_fe.csv\")\n","test_episodes = df_test_fe[\"game_episode\"].unique()\n","\n","test_dataset = EpisodeDataset(df_test_fe, test_episodes, max_len=270)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","\n","model = BiLSTMRegressor(input_dim=len(CONT_COLS)).to(device)\n","model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n","model.eval()\n","\n","preds = []\n","\n","with torch.no_grad():\n","    for x, mask, _ in test_loader:\n","        x, mask = x.to(device), mask.to(device)\n","        out = model(x, mask)\n","        preds.append(out.cpu().numpy()[0])\n","\n","preds = np.array(preds)\n","\n","sample = pd.read_csv(\"Data/sample_submission.csv\")\n","sample[\"end_x\"] = preds[:,0]\n","sample[\"end_y\"] = preds[:,1]\n","\n","sample.to_csv(\"Data/submission_1.csv\", index=False)\n","print(\"Saved submission.csv\")"],"metadata":{"id":"jnTqtVr-SSrB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10.4 BiLSTM+"],"metadata":{"id":"mEY1WhxqnoKy"}},{"cell_type":"markdown","source":["---\n","\n","    추가 Feature들\n","\n","| Column        | 의미                             |\n","| ------------- | ------------------------------ |\n","| event_simple  | Pass / Carry / Duel_Turnover 등 |\n","| result_simple | Success / Fail                 |\n","| zone_x        | D3 / M3 / A3                   |\n","| zone_y        | Left / Center / Right          |\n","| is_home       | home/away 구분                   |\n"],"metadata":{"id":"1Yk4mJbdnsyp"}},{"cell_type":"markdown","source":["### 10.4.1 Label Encoder 만들기"],"metadata":{"id":"rDtaVqeSojF_"}},{"cell_type":"markdown","source":["---\n","\n","    train_fe 전체를 기준으로 모든 categorical vocabulary를 결정"],"metadata":{"id":"Jh2FT-arom5e"}},{"cell_type":"code","source":["# 이벤트 레벨 Categorical Feature\n","CAT_COLS = [\"event_simple\", \"result_simple\", \"zone_x\", \"zone_y\", \"is_home\"]\n","\n","# 이 값들을 label encoding → embedding lookup → LSTM input에 concat\n","import pickle\n","\n","def build_label_encoders(df):\n","    encoders = {}\n","    num_classes = {}\n","\n","    for col in CAT_COLS:\n","        uniques = sorted(df[col].dropna().unique())\n","        encoders[col] = {u: i for i, u in enumerate(uniques)}\n","        num_classes[col] = len(uniques)\n","\n","    return encoders, num_classes\n","\n","encoders, num_classes = build_label_encoders(df_fe)\n","\n","# 저장\n","with open(\"Data/encoders.pkl\", \"wb\") as f:\n","    pickle.dump(encoders, f)\n","\n","with open(\"Data/num_classes.pkl\", \"wb\") as f:\n","    pickle.dump(num_classes, f)"],"metadata":{"id":"_3APYqETkJac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.4.2 EpisodeDataset을 categorical embedding과 함께 구축"],"metadata":{"id":"B6PlFp7_o3i7"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","class EpisodeDataset(Dataset):\n","    def __init__(self, df, episode_ids, max_len=270,\n","                 cont_cols=CONT_COLS, cat_cols=CAT_COLS, encoders=None):\n","\n","        self.max_len = max_len\n","        self.cont_cols = cont_cols\n","        self.cat_cols = cat_cols\n","        self.encoders = encoders\n","        self.episodes = []\n","\n","        sub = df[df[\"game_episode\"].isin(episode_ids)]\n","\n","        for ge, g in sub.groupby(\"game_episode\"):\n","            g = g.sort_values([\"time_seconds\", \"action_id\"]).reset_index(drop=True)\n","\n","            # 1) Numeric\n","            seq_cont = g[cont_cols].values.astype(\"float32\")\n","\n","            # 2) Categorical → label encoding\n","            seq_cat = []\n","            for col in cat_cols:\n","                seq_cat.append(g[col].map(encoders[col]).fillna(0).astype(int).values)\n","            seq_cat = np.vstack(seq_cat).T  # (T, C)\n","\n","            # padding\n","            seq_pad_cont, mask = pad_sequence(seq_cont, max_len)\n","\n","            pad_len = max_len - len(seq_cont)\n","            if pad_len > 0:\n","                pad_cat = np.zeros((pad_len, seq_cat.shape[1]), dtype=\"int64\")\n","                seq_pad_cat = np.vstack([seq_cat, pad_cat])\n","            else:\n","                seq_pad_cat = seq_cat[:max_len]\n","\n","            # target\n","            tx, ty = g[\"end_x\"].iloc[-1], g[\"end_y\"].iloc[-1]\n","            target = np.array([tx, ty], dtype=\"float32\")\n","\n","            self.episodes.append((\n","                torch.tensor(seq_pad_cont),\n","                torch.tensor(seq_pad_cat),\n","                torch.tensor(mask),\n","                torch.tensor(target)\n","            ))\n","\n","    def __len__(self):\n","        return len(self.episodes)\n","\n","    def __getitem__(self, idx):\n","        return self.episodes[idx]"],"metadata":{"id":"0SCIiQ3CoosX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.4.3 Collate 함수"],"metadata":{"id":"IHkLI2yhpCta"}},{"cell_type":"code","source":["def collate_fn(batch):\n","    x_cont_list = []\n","    x_cat_list = []\n","    mask_list = []\n","    y_list = []\n","\n","    for x_cont, x_cat, mask, target in batch:\n","        x_cont_list.append(x_cont)\n","        x_cat_list.append(x_cat)\n","        mask_list.append(mask)\n","        y_list.append(target)\n","\n","    x_cont = torch.stack(x_cont_list)\n","    x_cat = torch.stack(x_cat_list)\n","    mask = torch.stack(mask_list)\n","    y = torch.stack(y_list)\n","\n","    return x_cont, x_cat, mask, y"],"metadata":{"id":"6G9BNVjQo_As"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.4.4 Embedding + LSTM + FC 구조로 모델 구축"],"metadata":{"id":"2IcOlmjdpJI_"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","class BiLSTMWithCat(nn.Module):\n","    def __init__(self, cont_dim, num_classes_dict,\n","                 lstm_hidden=128, lstm_layers=1, dropout=0.2):\n","\n","        super().__init__()\n","\n","        self.cat_cols = list(num_classes_dict.keys())\n","\n","        # 각 카테고리마다 embedding layer 생성\n","        self.emb_layers = nn.ModuleDict({\n","            col: nn.Embedding(num_classes_dict[col], min(16, (num_classes_dict[col]+1)//2))\n","            for col in self.cat_cols\n","        })\n","\n","        emb_total_dim = sum(min(16, (num_classes_dict[c] + 1) // 2) for c in self.cat_cols)\n","\n","        input_dim = cont_dim + emb_total_dim\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_dim,\n","            hidden_size=lstm_hidden,\n","            num_layers=lstm_layers,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(lstm_hidden * 2, 128),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(128, 2)  # end_x, end_y\n","        )\n","\n","    def forward(self, x_cont, x_cat, mask):\n","        # embedding lookup: (B,T,C) → (B,T,emb_dim)\n","        emb_list = []\n","        for i, col in enumerate(self.cat_cols):\n","            emb_list.append(self.emb_layers[col](x_cat[:, :, i]))\n","        x_emb = torch.cat(emb_list, dim=-1)\n","\n","        # concat numeric + embedding\n","        x = torch.cat([x_cont, x_emb], dim=-1)\n","\n","        # pack sequence\n","        lengths = mask.sum(dim=1).long()\n","        lengths_sorted, sort_idx = lengths.sort(descending=True)\n","        x_sorted = x[sort_idx]\n","\n","        packed = pack_padded_sequence(\n","            x_sorted,\n","            lengths_sorted.cpu(),\n","            batch_first=True,\n","            enforce_sorted=True\n","        )\n","\n","        _, (h_n, _) = self.lstm(packed)\n","\n","        h_fwd = h_n[-2]\n","        h_bwd = h_n[-1]\n","        h = torch.cat([h_fwd, h_bwd], dim=-1)\n","\n","        _, inv_idx = sort_idx.sort()\n","        h = h[inv_idx]\n","\n","        return self.fc(h)"],"metadata":{"id":"R2UEbfjqpFEF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.4.5 DataLoader 준비"],"metadata":{"id":"v0MJYJFapT0H"}},{"cell_type":"code","source":["train_dataset = EpisodeDataset(df_fe, train_epis, max_len=270,\n","                               cont_cols=CONT_COLS, cat_cols=CAT_COLS, encoders=encoders)\n","\n","valid_dataset = EpisodeDataset(df_fe, valid_epis, max_len=270,\n","                               cont_cols=CONT_COLS, cat_cols=CAT_COLS, encoders=encoders)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"],"metadata":{"id":"T_-Vi34hpRl_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.4.6 Train Loop"],"metadata":{"id":"R4Vsgm7vpyKQ"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"device =\", device)\n","\n","model = BiLSTMWithCat(\n","    cont_dim=len(CONT_COLS),\n","    num_classes_dict=num_classes\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()\n","\n","def euclidean(pred, target):\n","    return torch.sqrt(((pred - target)**2).sum(dim=1)).mean().item()\n","\n","EPOCHS = 50\n","\n","for epoch in range(EPOCHS):\n","    # ----------------------- Train -----------------------\n","    model.train()\n","    train_loss = 0\n","\n","    for x_cont, x_cat, mask, y in train_loader:\n","        x_cont = x_cont.to(device)\n","        x_cat = x_cat.to(device)\n","        mask = mask.to(device)\n","        y = y.to(device)\n","\n","        pred = model(x_cont, x_cat, mask)\n","        loss = criterion(pred, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    # ----------------------- Valid -----------------------\n","    model.eval()\n","    val_loss = 0\n","    val_dist = 0\n","\n","    with torch.no_grad():\n","        for x_cont, x_cat, mask, y in valid_loader:\n","            x_cont = x_cont.to(device)\n","            x_cat = x_cat.to(device)\n","            mask = mask.to(device)\n","            y = y.to(device)\n","\n","            pred = model(x_cont, x_cat, mask)\n","            loss = criterion(pred, y)\n","\n","            val_loss += loss.item()\n","            val_dist += euclidean(pred, y)\n","\n","    print(\n","        f\"[Epoch {epoch}] \"\n","        f\"TrainLoss={train_loss:.4f} | \"\n","        f\"ValidLoss={val_loss:.4f} | \"\n","        f\"Euclid={val_dist:.4f}\"\n","    )\n","\n","torch.save(model.state_dict(), \"model.pt\")\n","print(\"Saved model!\")"],"metadata":{"id":"GEPFO9EApXkU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","[Epoch 20] TrainLoss=70213.4230 | ValidLoss=16892.3024 | Euclid=1510.8061\n","[Epoch 21] TrainLoss=70222.9263 | ValidLoss=16957.8709 | Euclid=1500.0777\n","[Epoch 22] TrainLoss=70127.4421 | ValidLoss=17092.4315 | Euclid=1522.6829\n","[Epoch 23] TrainLoss=69660.9905 | ValidLoss=16847.7885 | Euclid=1509.6669\n","[Epoch 24] TrainLoss=68723.5326 | ValidLoss=16813.4463 | Euclid=1511.7533\n","[Epoch 25] TrainLoss=68966.2243 | ValidLoss=16857.4438 | Euclid=1491.5315\n","[Epoch 26] TrainLoss=68657.3103 | ValidLoss=17128.0384 | Euclid=1532.2479\n","[Epoch 27] TrainLoss=68148.0404 | ValidLoss=16771.6007 | Euclid=1477.3296\n","[Epoch 28] TrainLoss=67543.9454 | ValidLoss=16825.2638 | Euclid=1496.1501\n","[Epoch 29] TrainLoss=66826.2477 | ValidLoss=16519.7088 | Euclid=1479.4009\n","```\n","\n"],"metadata":{"id":"rCN0hvwgt9Q_"}},{"cell_type":"markdown","source":["## 10.5 마지막 3-step 추가한 버전으로 한 번 더"],"metadata":{"id":"1u7mRld9uXDf"}},{"cell_type":"markdown","source":["---\n","\n","    이번에 추가할 Feature\n","\n","| Feature                      | 의미      |\n","| ---------------------------- | ------- |\n","| dx_last1, dx_last2, dx_last3 | 마지막 이동량 |\n","| dy_last1, dy_last2, dy_last3 | 마지막 이동량 |\n","| dist_last1..3                | 마지막 거리  |\n","| angle_last1..3               | 마지막 변화각 |\n","| event_simple_last1..3        | 이벤트 종류  |\n","| result_simple_last1..3       | 성공/실패   |"],"metadata":{"id":"0iHa0CPCubHL"}},{"cell_type":"code","source":["def add_last_k_features(df, k=3):\n","    df = df.copy()\n","\n","    records = []\n","\n","    for ge, g in df.groupby(\"game_episode\"):\n","        g = g.reset_index(drop=True)\n","\n","        # 마지막 k개의 row만 추출\n","        last_rows = g.tail(k)\n","        last_rows = last_rows.reset_index(drop=True)\n","\n","        rec = {\"game_episode\": ge}\n","\n","        # numeric last-k\n","        for i in range(k):\n","            if i < len(last_rows):\n","                rec[f\"dx_last{i+1}\"] = last_rows.loc[i, \"dx\"]\n","                rec[f\"dy_last{i+1}\"] = last_rows.loc[i, \"dy\"]\n","                rec[f\"dist_last{i+1}\"] = last_rows.loc[i, \"distance\"]\n","                rec[f\"angle_last{i+1}\"] = last_rows.loc[i, \"angle\"]\n","            else:\n","                rec[f\"dx_last{i+1}\"] = 0\n","                rec[f\"dy_last{i+1}\"] = 0\n","                rec[f\"dist_last{i+1}\"] = 0\n","                rec[f\"angle_last{i+1}\"] = 0\n","\n","        # categorical last-k\n","        for i in range(k):\n","            if i < len(last_rows):\n","                rec[f\"event_last{i+1}\"] = last_rows.loc[i, \"event_simple\"]\n","                rec[f\"result_last{i+1}\"] = last_rows.loc[i, \"result_simple\"]\n","            else:\n","                rec[f\"event_last{i+1}\"] = \"None\"\n","                rec[f\"result_last{i+1}\"] = \"None\"\n","\n","        records.append(rec)\n","\n","    return pd.DataFrame(records)"],"metadata":{"id":"MjDPKnOUt-JW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epi_last3 = add_last_k_features(df_fe, k=3)\n","\n","epi_full = epi_fe.merge(epi_last3, on=\"game_episode\", how=\"left\")\n","epi_full.to_csv(\"Data/train_epi_full.csv\", index=False)\n","epi_full.head()"],"metadata":{"id":"yWww8XsBuocL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.5.1 추가 컬럼 정의"],"metadata":{"id":"3a76FwCfv6-u"}},{"cell_type":"code","source":["# episode-level numeric feature (epi_full 기준 컬럼들)\n","EPI_COLS = [\n","    \"epi_len\",\n","    \"ratio_pass\", \"ratio_carry\", \"ratio_turnover\",\n","    \"dx_mean\", \"dy_mean\",\n","    \"angle_mean\", \"angle_std\",\n","    \"dist_mean\", \"dist_cum\",\n","    \"angle_change_std\", \"angle_change_mean_abs\",\n","    \"angle_change_max\", \"angle_change_N\",\n","    \"entropy_event\",\n","\n","    # last-3 step numeric 요약\n","    \"dx_last1\", \"dy_last1\", \"dist_last1\", \"angle_last1\",\n","    \"dx_last2\", \"dy_last2\", \"dist_last2\", \"angle_last2\",\n","    \"dx_last3\", \"dy_last3\", \"dist_last3\", \"angle_last3\",\n","]"],"metadata":{"id":"gdUJEWZPv-ED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.5.2 EpisodeHybridDataset"],"metadata":{"id":"P33lEUsBwPiS"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","class EpisodeHybridDataset(Dataset):\n","    def __init__(\n","        self,\n","        df_fe,         # 이벤트 레벨 FE (train_fe.csv or test_fe.csv)\n","        epi_full,      # 에피소드 레벨 FE (train_epi_full.csv or test_epi_full.csv)\n","        episode_ids,   # 사용할 episode 리스트\n","        encoders,\n","        max_len=270,\n","        cont_cols=CONT_COLS,\n","        cat_cols=CAT_COLS,\n","        epi_cols=EPI_COLS,\n","        has_target=True,\n","    ):\n","        self.max_len = max_len\n","        self.cont_cols = cont_cols\n","        self.cat_cols = cat_cols\n","        self.epi_cols = epi_cols\n","        self.encoders = encoders\n","        self.has_target = has_target\n","\n","        # episode-level feature는 index를 game_episode로 설정해두면 lookup이 빠름\n","        self.epi_full = epi_full.set_index(\"game_episode\")\n","\n","        self.episodes = []\n","        sub = df_fe[df_fe[\"game_episode\"].isin(episode_ids)]\n","\n","        for ge, g in sub.groupby(\"game_episode\"):\n","            g = g.sort_values([\"time_seconds\", \"action_id\"]).reset_index(drop=True)\n","\n","            # 1) sequence numeric\n","            seq_cont = g[cont_cols].values.astype(\"float32\")\n","\n","            # 2) sequence categorical → index\n","            seq_cat = []\n","            for col in cat_cols:\n","                seq_cat.append(\n","                    g[col].map(encoders[col]).fillna(0).astype(int).values\n","                )\n","            seq_cat = np.vstack(seq_cat).T  # (T, C)\n","\n","            # 3) padding\n","            seq_pad_cont, mask = pad_sequence(seq_cont, max_len)\n","\n","            pad_len = max_len - len(seq_cont)\n","            if pad_len > 0:\n","                pad_cat = np.zeros((pad_len, seq_cat.shape[1]), dtype=\"int64\")\n","                seq_pad_cat = np.vstack([seq_cat, pad_cat])\n","            else:\n","                seq_pad_cat = seq_cat[:max_len]\n","\n","            # 4) episode-level feature\n","            epi_vec = self.epi_full.loc[ge, epi_cols].values.astype(\"float32\")\n","\n","            # 5) target (train/valid에서만 유효)\n","            if has_target:\n","                tx, ty = g[\"end_x\"].iloc[-1], g[\"end_y\"].iloc[-1]\n","                target = np.array([tx, ty], dtype=\"float32\")\n","            else:\n","                target = np.array([0.0, 0.0], dtype=\"float32\")  # dummy\n","\n","            self.episodes.append((\n","                torch.tensor(seq_pad_cont),   # (T, F_cont)\n","                torch.tensor(seq_pad_cat),    # (T, C_cat)\n","                torch.tensor(mask),           # (T,)\n","                torch.tensor(epi_vec),        # (F_epi,)\n","                torch.tensor(target)          # (2,)\n","            ))\n","\n","    def __len__(self):\n","        return len(self.episodes)\n","\n","    def __getitem__(self, idx):\n","        return self.episodes[idx]"],"metadata":{"id":"dcFdlf6vwRRb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.5.3 Collate 함수"],"metadata":{"id":"7-QrR56RwWoy"}},{"cell_type":"code","source":["def collate_hybrid(batch):\n","    x_cont_list = []\n","    x_cat_list = []\n","    mask_list = []\n","    epi_list = []\n","    y_list = []\n","\n","    for x_cont, x_cat, mask, epi_vec, target in batch:\n","        x_cont_list.append(x_cont)\n","        x_cat_list.append(x_cat)\n","        mask_list.append(mask)\n","        epi_list.append(epi_vec)\n","        y_list.append(target)\n","\n","    x_cont = torch.stack(x_cont_list)  # (B, T, F_cont)\n","    x_cat = torch.stack(x_cat_list)    # (B, T, C_cat)\n","    mask = torch.stack(mask_list)      # (B, T)\n","    epi = torch.stack(epi_list)        # (B, F_epi)\n","    y = torch.stack(y_list)            # (B, 2)\n","\n","    return x_cont, x_cat, mask, epi, y"],"metadata":{"id":"AInbLGipwYA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.5.4 HybridBiLSTM"],"metadata":{"id":"9_8A5jxywbZi"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","class HybridBiLSTMWithCat(nn.Module):\n","    def __init__(self, cont_dim, num_classes_dict,\n","                 epi_feat_dim, lstm_hidden=128, lstm_layers=1, dropout=0.2):\n","        super().__init__()\n","\n","        self.cat_cols = list(num_classes_dict.keys())\n","\n","        # 카테고리별 embedding layer\n","        self.emb_layers = nn.ModuleDict({\n","            col: nn.Embedding(num_classes_dict[col], min(16, (num_classes_dict[col] + 1) // 2))\n","            for col in self.cat_cols\n","        })\n","\n","        emb_dim = sum(min(16, (n + 1) // 2) for n in num_classes_dict.values())\n","        input_dim = cont_dim + emb_dim\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_dim,\n","            hidden_size=lstm_hidden,\n","            num_layers=lstm_layers,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(lstm_hidden * 2 + epi_feat_dim, 256),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(256, 2)  # end_x, end_y\n","        )\n","\n","    def forward(self, x_cont, x_cat, mask):\n","        \"\"\"\n","        x_cont: (B, T, F_cont)\n","        x_cat:  (B, T, C_cat)\n","        mask:   (B, T)\n","        \"\"\"\n","        # 1) embedding lookup\n","        emb_list = []\n","        for i, col in enumerate(self.cat_cols):\n","            emb_list.append(self.emb_layers[col](x_cat[:, :, i]))\n","        x_emb = torch.cat(emb_list, dim=-1)  # (B, T, emb_dim)\n","\n","        # 2) concat numeric + embedding\n","        x = torch.cat([x_cont, x_emb], dim=-1)  # (B, T, input_dim)\n","\n","        # 3) pack sequence\n","        lengths = mask.sum(dim=1).long()\n","        lengths_sorted, sort_idx = lengths.sort(descending=True)\n","        x_sorted = x[sort_idx]\n","\n","        packed = pack_padded_sequence(\n","            x_sorted,\n","            lengths_sorted.cpu(),\n","            batch_first=True,\n","            enforce_sorted=True\n","        )\n","\n","        _, (h_n, _) = self.lstm(packed)\n","\n","        # bidirectional → 마지막 layer의 forward/backward hidden state\n","        h_fwd = h_n[-2]\n","        h_bwd = h_n[-1]\n","        h = torch.cat([h_fwd, h_bwd], dim=-1)  # (B, 2*lstm_hidden)\n","\n","        # 원래 순서로 되돌리기\n","        _, inv_idx = sort_idx.sort()\n","        h = h[inv_idx]\n","\n","        return h  # FC 전에 episde feature와 concat할 raw representation"],"metadata":{"id":"EEeBGyRhwfLc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.5.5 DataLoader 준비"],"metadata":{"id":"wtdqrAE9wrzN"}},{"cell_type":"code","source":["df_fe = pd.read_csv(\"Data/train_fe.csv\")\n","epi_full = pd.read_csv(\"Data/train_epi_full.csv\")\n","\n","with open(\"Data/encoders.pkl\", \"rb\") as f:\n","    encoders = pickle.load(f)\n","with open(\"Data/num_classes.pkl\", \"rb\") as f:\n","    num_classes = pickle.load(f)\n","\n","MAX_LEN = 270\n","\n","train_dataset = EpisodeHybridDataset(\n","    df_fe=df_fe,\n","    epi_full=epi_full,\n","    episode_ids=train_epis,\n","    encoders=encoders,\n","    max_len=MAX_LEN,\n","    cont_cols=CONT_COLS,\n","    cat_cols=CAT_COLS,\n","    epi_cols=EPI_COLS,\n","    has_target=True,\n",")\n","\n","valid_dataset = EpisodeHybridDataset(\n","    df_fe=df_fe,\n","    epi_full=epi_full,\n","    episode_ids=valid_epis,\n","    encoders=encoders,\n","    max_len=MAX_LEN,\n","    cont_cols=CONT_COLS,\n","    cat_cols=CAT_COLS,\n","    epi_cols=EPI_COLS,\n","    has_target=True,\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,  collate_fn=collate_hybrid)\n","valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_hybrid)"],"metadata":{"id":"z1CS8YpDwmKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.5.6 Train Loop"],"metadata":{"id":"idH9Eudrw7Hb"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"device =\", device)\n","\n","epi_feat_dim = len(EPI_COLS)\n","\n","backbone = HybridBiLSTMWithCat(\n","    cont_dim=len(CONT_COLS),\n","    num_classes_dict=num_classes,\n","    epi_feat_dim=epi_feat_dim,   # 여기서는 FC 밖에서 concat할 거라 안 써도 되지만, 일단 유지\n",").to(device)\n","\n","# FC를 밖에서 한 번 더 감싼 구조로 가자\n","fc_head = nn.Sequential(\n","    nn.Linear(2 * 128 + epi_feat_dim, 256),  # hidden_dim=128 가정\n","    nn.ReLU(),\n","    nn.Dropout(0.2),\n","    nn.Linear(256, 2)\n",").to(device)\n","\n","params = list(backbone.parameters()) + list(fc_head.parameters())\n","optimizer = torch.optim.AdamW(params, lr=1e-3)\n","criterion = nn.MSELoss()\n","\n","def euclidean(pred, target):\n","    return torch.sqrt(((pred - target) ** 2).sum(dim=1)).mean().item()\n","\n","EPOCHS = 50\n","\n","for epoch in range(EPOCHS):\n","    # ---------- Train ----------\n","    backbone.train()\n","    fc_head.train()\n","    train_loss = 0.0\n","\n","    for x_cont, x_cat, mask, epi_vec, y in train_loader:\n","        x_cont = x_cont.to(device)\n","        x_cat  = x_cat.to(device)\n","        mask   = mask.to(device)\n","        epi_vec = epi_vec.to(device)\n","        y      = y.to(device)\n","\n","        h = backbone(x_cont, x_cat, mask)      # (B, 2*hidden)\n","        z = torch.cat([h, epi_vec], dim=1)     # (B, 2*hidden + epi_feat_dim)\n","        pred = fc_head(z)                      # (B, 2)\n","\n","        loss = criterion(pred, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    # ---------- Valid ----------\n","    backbone.eval()\n","    fc_head.eval()\n","    val_loss = 0.0\n","    val_dist = 0.0\n","\n","    with torch.no_grad():\n","        for x_cont, x_cat, mask, epi_vec, y in valid_loader:\n","            x_cont = x_cont.to(device)\n","            x_cat  = x_cat.to(device)\n","            mask   = mask.to(device)\n","            epi_vec = epi_vec.to(device)\n","            y      = y.to(device)\n","\n","            h = backbone(x_cont, x_cat, mask)\n","            z = torch.cat([h, epi_vec], dim=1)\n","            pred = fc_head(z)\n","\n","            loss = criterion(pred, y)\n","            val_loss += loss.item()\n","            val_dist += euclidean(pred, y)\n","\n","    print(\n","        f\"[Epoch {epoch}] \"\n","        f\"TrainLoss={train_loss:.4f} | \"\n","        f\"ValidLoss={val_loss:.4f} | \"\n","        f\"Euclid={val_dist:.4f}\"\n","    )\n","\n","torch.save({\n","    \"backbone\": backbone.state_dict(),\n","    \"fc_head\": fc_head.state_dict(),\n","}, \"Data/hybrid_model.pt\")\n","\n","print(\"Saved hybrid_model.pt\")"],"metadata":{"id":"aH0QyigHwuDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","[Epoch 39] TrainLoss=59771.0182 | ValidLoss=17011.9001 | Euclid=1487.1916 ⭐\n","[Epoch 40] TrainLoss=59688.4913 | ValidLoss=17203.0105 | Euclid=1512.8117\n","[Epoch 41] TrainLoss=59036.6915 | ValidLoss=17134.9613 | Euclid=1493.1779\n","[Epoch 42] TrainLoss=57437.4887 | ValidLoss=17217.3898 | Euclid=1501.6665\n","[Epoch 43] TrainLoss=58409.0617 | ValidLoss=17064.9690 | Euclid=1507.6640\n","[Epoch 44] TrainLoss=57272.1718 | ValidLoss=17342.1851 | Euclid=1502.4958\n","[Epoch 45] TrainLoss=57427.6512 | ValidLoss=18147.3226 | Euclid=1552.6433\n","[Epoch 46] TrainLoss=56295.2120 | ValidLoss=17060.9277 | Euclid=1494.9201\n","[Epoch 47] TrainLoss=55884.3484 | ValidLoss=17310.6341 | Euclid=1499.5079\n","[Epoch 48] TrainLoss=54851.3513 | ValidLoss=18033.2919 | Euclid=1529.1722\n","[Epoch 49] TrainLoss=54920.3710 | ValidLoss=17347.7805 | Euclid=1508.3866\n","```\n","\n"],"metadata":{"id":"xWDfZmjgyIOi"}},{"cell_type":"markdown","source":["## 10.6 Transformer Encoder Baseline"],"metadata":{"id":"H_t87pBw1SI4"}},{"cell_type":"markdown","source":["### 10.6.1 Padding & Dataset"],"metadata":{"id":"tu9GxwQx1Zbo"}},{"cell_type":"code","source":["import numpy as np\n","\n","def pad_sequence_cont_cat(seq_cont, seq_cat, max_len):\n","    \"\"\"\n","    seq_cont: (T, F_cont)\n","    seq_cat:  (T, C_cat)\n","    return:\n","      padded_cont: (max_len, F_cont)\n","      padded_cat:  (max_len, C_cat)\n","      mask:        (max_len,)  1: valid, 0: pad\n","    \"\"\"\n","    T = seq_cont.shape[0]\n","    pad_len = max_len - T\n","\n","    if pad_len > 0:\n","        pad_cont = np.zeros((pad_len, seq_cont.shape[1]), dtype=\"float32\")\n","        pad_cat = np.zeros((pad_len, seq_cat.shape[1]), dtype=\"int64\")\n","        padded_cont = np.vstack([seq_cont, pad_cont])\n","        padded_cat = np.vstack([seq_cat, pad_cat])\n","        mask = np.concatenate([np.ones(T), np.zeros(pad_len)])\n","    else:\n","        padded_cont = seq_cont[:max_len]\n","        padded_cat = seq_cat[:max_len]\n","        mask = np.ones(max_len)\n","\n","    return padded_cont.astype(\"float32\"), padded_cat.astype(\"int64\"), mask.astype(\"float32\")\n","\n","def build_label_encoders(df):\n","    encoders = {}\n","    num_classes = {}\n","\n","    for col in CAT_COLS:\n","        uniques = sorted(df[col].dropna().unique())\n","        encoders[col] = {u: (i+1) for i, u in enumerate(uniques)}  # PAD=0 reserved\n","        num_classes[col] = len(uniques) + 1\n","\n","    return encoders, num_classes"],"metadata":{"id":"Jt6xaSX2xAaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class EpisodeDatasetWithCat(Dataset):\n","    \"\"\"\n","    train / valid용 Episode Dataset (target 포함)\n","    \"\"\"\n","    def __init__(\n","        self,\n","        df,\n","        episode_ids,\n","        max_len=270,\n","        cont_cols=CONT_COLS,\n","        cat_cols=CAT_COLS,\n","        encoders=None,\n","    ):\n","        self.max_len = max_len\n","        self.cont_cols = cont_cols\n","        self.cat_cols = cat_cols\n","        self.encoders = encoders\n","        self.episodes = []\n","\n","        sub = df[df[\"game_episode\"].isin(episode_ids)]\n","\n","        for ge, g in sub.groupby(\"game_episode\"):\n","            g = g.sort_values([\"time_seconds\", \"action_id\"]).reset_index(drop=True)\n","\n","            # 1) numeric\n","            seq_cont = g[cont_cols].values.astype(\"float32\")\n","\n","            # 2) categorical → label index\n","            seq_cat_list = []\n","            for col in cat_cols:\n","                seq_cat_list.append(\n","                    g[col].map(encoders[col]).fillna(0).astype(int).values\n","                )\n","            seq_cat = np.vstack(seq_cat_list).T  # (T, C)\n","\n","            # 3) padding\n","            pad_cont, pad_cat, mask = pad_sequence_cont_cat(seq_cont, seq_cat, max_len)\n","\n","            # 4) target (마지막 패스 end_x, end_y)\n","            tx, ty = g[\"end_x\"].iloc[-1], g[\"end_y\"].iloc[-1]\n","            target = np.array([tx, ty], dtype=\"float32\")\n","\n","            self.episodes.append(\n","                (\n","                    torch.tensor(pad_cont),    # (T, F_cont)\n","                    torch.tensor(pad_cat),     # (T, C_cat)\n","                    torch.tensor(mask),        # (T,)\n","                    torch.tensor(target),      # (2,)\n","                )\n","            )\n","\n","    def __len__(self):\n","        return len(self.episodes)\n","\n","    def __getitem__(self, idx):\n","        return self.episodes[idx]"],"metadata":{"id":"qUai-6-G1dHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn_cat(batch):\n","    x_cont_list = []\n","    x_cat_list = []\n","    mask_list = []\n","    y_list = []\n","\n","    for x_cont, x_cat, mask, target in batch:\n","        x_cont_list.append(x_cont)\n","        x_cat_list.append(x_cat)\n","        mask_list.append(mask)\n","        y_list.append(target)\n","\n","    x_cont = torch.stack(x_cont_list)   # (B, T, F_cont)\n","    x_cat  = torch.stack(x_cat_list)    # (B, T, C_cat)\n","    mask   = torch.stack(mask_list)     # (B, T)\n","    y      = torch.stack(y_list)        # (B, 2)\n","\n","    return x_cont, x_cat, mask, y"],"metadata":{"id":"Pl5KCuxr1f9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","MAX_LEN = 270\n","\n","train_dataset = EpisodeDatasetWithCat(\n","    df_fe,\n","    train_epis,\n","    max_len=MAX_LEN,\n","    cont_cols=CONT_COLS,\n","    cat_cols=CAT_COLS,\n","    encoders=encoders,\n",")\n","\n","valid_dataset = EpisodeDatasetWithCat(\n","    df_fe,\n","    valid_epis,\n","    max_len=MAX_LEN,\n","    cont_cols=CONT_COLS,\n","    cat_cols=CAT_COLS,\n","    encoders=encoders,\n",")\n","\n","train_loader = DataLoader(\n","    train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn_cat\n",")\n","valid_loader = DataLoader(\n","    valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_cat\n",")"],"metadata":{"id":"KFzbLKa-1h-h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.6.2 Sinusoidal Positional Encoding"],"metadata":{"id":"N2HOFab-1nrM"}},{"cell_type":"code","source":["import math\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=300):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)  # (T, D)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (T,1)\n","        div_term = torch.exp(\n","            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n","        )\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        pe = pe.unsqueeze(0)  # (1, T, D)\n","        self.register_buffer(\"pe\", pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (B, T, D)\n","        \"\"\"\n","        T = x.size(1)\n","        x = x + self.pe[:, :T, :]\n","        return x"],"metadata":{"id":"0tRdBjTd1kdi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.6.3 Transformer Encoder + Cat Embedding"],"metadata":{"id":"vEWkXlni1wwZ"}},{"cell_type":"code","source":["class TransformerFinalPassRegressor(nn.Module):\n","    def __init__(\n","        self,\n","        cont_dim,\n","        num_classes_dict,\n","        d_model=128,\n","        nhead=4,\n","        num_layers=2,\n","        dim_feedforward=256,\n","        dropout=0.2,\n","        max_len=300,\n","    ):\n","        super().__init__()\n","\n","        self.cat_cols = list(num_classes_dict.keys())\n","\n","        # 1) 각 categorical feature embedding\n","        self.emb_layers = nn.ModuleDict()\n","        emb_dims = []\n","        for col, n in num_classes_dict.items():\n","            emb_dim = min(16, (n + 1) // 2)\n","            self.emb_layers[col] = nn.Embedding(n, emb_dim)\n","            emb_dims.append(emb_dim)\n","\n","        self.emb_total_dim = sum(emb_dims)\n","\n","        # 2) numeric + embedding concat → d_model로 projection\n","        self.input_proj = nn.Linear(cont_dim + self.emb_total_dim, d_model)\n","\n","        # 3) positional encoding\n","        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n","\n","        # 4) Transformer Encoder\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True,  # (B, T, D)\n","        )\n","        self.transformer = nn.TransformerEncoder(\n","            encoder_layer,\n","            num_layers=num_layers,\n","        )\n","\n","        # 5) 출력 헤드 (masked mean pooling → 회귀)\n","        self.fc = nn.Sequential(\n","            nn.Linear(d_model, 128),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(128, 2),  # end_x, end_y\n","        )\n","\n","    def forward(self, x_cont, x_cat, mask):\n","        \"\"\"\n","        x_cont: (B, T, F_cont)\n","        x_cat:  (B, T, C_cat)  (각 column이 index)\n","        mask:   (B, T)         1: valid, 0: pad\n","        \"\"\"\n","        B, T, _ = x_cont.size()\n","\n","        # 1) categorical embedding\n","        emb_list = []\n","        for i, col in enumerate(self.cat_cols):\n","            # x_cat[:,:,i] : (B, T)\n","            emb = self.emb_layers[col](x_cat[:, :, i])  # (B, T, emb_dim)\n","            emb_list.append(emb)\n","        x_emb = torch.cat(emb_list, dim=-1)  # (B, T, sum_emb)\n","\n","        # 2) concat numeric + embedding\n","        x = torch.cat([x_cont, x_emb], dim=-1)  # (B, T, cont+emb)\n","        x = self.input_proj(x)                  # (B, T, d_model)\n","\n","        # 3) positional encoding\n","        x = self.pos_encoder(x)                 # (B, T, d_model)\n","\n","        # 4) key_padding_mask: True → 무시할 위치 (pad)\n","        # 현재 mask: 1(valid), 0(pad) 이므로 반전\n","        key_padding_mask = (mask == 0)          # (B, T), bool\n","\n","        # 5) Transformer Encoder\n","        x_enc = self.transformer(\n","            x, src_key_padding_mask=key_padding_mask\n","        )  # (B, T, d_model)\n","\n","        # 6) masked mean pooling\n","        mask_f = mask.unsqueeze(-1)             # (B, T, 1)\n","        x_enc_masked = x_enc * mask_f           # pad 위치는 0\n","        sum_enc = x_enc_masked.sum(dim=1)       # (B, d_model)\n","        len_valid = mask_f.sum(dim=1).clamp(min=1e-6)  # (B,1)\n","        pooled = sum_enc / len_valid\n","\n","        # 7) regression head\n","        out = self.fc(pooled)                   # (B, 2)\n","        return out"],"metadata":{"id":"M8QD2F2k1vD3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.6.4 Train Loop"],"metadata":{"id":"LjfmDMja12CB"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"device =\", device)\n","\n","model = TransformerFinalPassRegressor(\n","    cont_dim=len(CONT_COLS),\n","    num_classes_dict=num_classes,\n","    d_model=128,\n","    nhead=4,\n","    num_layers=3,        # 2~3 정도부터 시작해봐도 좋음\n","    dim_feedforward=256,\n","    dropout=0.2,\n","    max_len=MAX_LEN + 5, # 여유 있게\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n","criterion = nn.MSELoss()\n","\n","def euclidean(pred, target):\n","    # pred, target: (B, 2)\n","    return torch.sqrt(((pred - target) ** 2).sum(dim=1)).mean().item()\n","\n","EPOCHS = 50\n","\n","for epoch in range(EPOCHS):\n","    # -------- Train --------\n","    model.train()\n","    train_loss = 0.0\n","\n","    for x_cont, x_cat, mask, y in train_loader:\n","        x_cont = x_cont.to(device)\n","        x_cat  = x_cat.to(device)\n","        mask   = mask.to(device)\n","        y      = y.to(device)\n","\n","        pred = model(x_cont, x_cat, mask)\n","        loss = criterion(pred, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # gradient clipping (Transformer에서 종종 도움 됨)\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    # -------- Valid --------\n","    model.eval()\n","    val_loss = 0.0\n","    val_dist = 0.0\n","\n","    with torch.no_grad():\n","        for x_cont, x_cat, mask, y in valid_loader:\n","            x_cont = x_cont.to(device)\n","            x_cat  = x_cat.to(device)\n","            mask   = mask.to(device)\n","            y      = y.to(device)\n","\n","            pred = model(x_cont, x_cat, mask)\n","            loss = criterion(pred, y)\n","\n","            val_loss += loss.item()\n","            val_dist += euclidean(pred, y)\n","\n","    print(\n","        f\"[Epoch {epoch}] \"\n","        f\"TrainLoss={train_loss:.4f} | \"\n","        f\"ValidLoss={val_loss:.4f} | \"\n","        f\"Euclid={val_dist:.4f}\"\n","    )\n","\n","torch.save(model.state_dict(), \"Data/model_transformer.pt\")\n","\n","print(\"Saved Transformer model!\")"],"metadata":{"id":"g1WOX0Wo1z_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UXMG5dFV14j6"},"execution_count":null,"outputs":[]}]}